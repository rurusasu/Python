{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# 相対パスを書くために、現在のディレクトリの位置を書き出す\n",
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def data_load(path, img_width, img_height, CLS, Framework='Tensorflow'):\n",
    "    \"\"\"ファイルパスからデータセットをロードするための関数\"\"\"\n",
    "    xs = [] # 訓練データ用の空リスト\n",
    "    ts = [] # データラベル用の空リスト\n",
    "    paths = [] # ロードしたファイルパス用の空リスト\n",
    "    img_read_err = [] # ロードできなかったファイルパス用の空リスト\n",
    "    num_classes = len(CLS)+1\n",
    "\n",
    "    # データをロードするためにディレクトリを下りていく\n",
    "    for dir_path_1 in glob(DirPath):\n",
    "        for dir_path_2 in glob(dir_path_1 + '/*'):\n",
    "            for path in glob(dir_path_2 + '/*'):\n",
    "                #print(path, 'を読み込みました。')\n",
    "\n",
    "                # 訓練用画像を読み込む\n",
    "                x = cv2.imread(path)\n",
    "                if x is None: # もし，画像がロードできなかった場合\n",
    "                    print(path, 'を読み込めませんでした．')\n",
    "                    img_read_err.append(path)\n",
    "                    continue\n",
    "                else: # 画像がロードできた場合\n",
    "                    x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n",
    "                    x /= 255.\n",
    "                    xs.append(x)\n",
    "                \n",
    "                # 正解ラベルを作成する\n",
    "                lbl = dir_path_1[dir_path_1.find('\\\\'):].strip('\\\\')\n",
    "                # 使用するフレームワークによって，正解ラベルの作成方法が異なる\n",
    "                if (Framework is 'PyTorch'):\n",
    "                    t = float(lbl)\n",
    "                elif (Framework is 'Tensorflow') or (Framework is 'Keras'):\n",
    "                    # one-hot-labelを作成する\n",
    "                    t = np.zeros(num_classes)\n",
    "                    for i, cls in enumerate(CLS):\n",
    "                        if cls == lbl:\n",
    "                            t[i] = 1\n",
    "\n",
    "                ts.append(t)\n",
    "                paths.append(path)\n",
    "\n",
    "    xs = np.array(xs, dtype=np.float32)\n",
    "    ts = np.array(ts, dtype=np.int)\n",
    "\n",
    "    return xs, ts, paths, img_read_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNetを作成する\n",
    "## Network\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, Input, BatchNormalization\n",
    "\n",
    "def LeNet(img_width, img_height, num_classes):\n",
    "    inputs = Input((img_height, img_width, 3))\n",
    "    x = Conv2D(6, (5, 5), padding='valid', activation=None, name='conv1')(inputs)\n",
    "    x = MaxPool2D((2, 2), padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    x = Conv2D(16, (5, 5), padding='valid', activation=None, name='conv2')(x)\n",
    "    x = MaxPool2D((2, 2), padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(120, name='dense1', activation=None)(x)\n",
    "    x = Dense(64, name='dense2', activation=None)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name='model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(DirPath, img_size, cls_label):\n",
    "    model = LeNet(img_size[0], img_size[1], len(cls_label)+1)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    xs, ts, paths, _ = data_load(DirPath, img_size[0], img_size[1], cls_label)\n",
    "\n",
    "    # training\n",
    "    mb = 100\n",
    "    mbi = 0\n",
    "    count = 1\n",
    "    loss_ls = []\n",
    "\n",
    "    loss_ave = 0\n",
    "    loss_AveMin = 1\n",
    "    loss_AveCnt = 0\n",
    "    #loss_AveLs = []\n",
    "\n",
    "    train_ind = np.arange(len(xs))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(train_ind)\n",
    "\n",
    "    #for i in range(500):\n",
    "    while True:\n",
    "        if mbi + mb > len(xs):\n",
    "            mb_ind = train_ind[mbi:]\n",
    "            np.random.shuffle(train_ind)\n",
    "            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n",
    "            mbi = mb - (len(xs) - mbi)\n",
    "            \n",
    "            loss_ave = np.average(loss_ls)\n",
    "            if loss_AveMin > loss_ave:\n",
    "                loss_AveMin = loss_ave\n",
    "                loss_AveCnt = 0\n",
    "            else: loss_AveCnt += 1\n",
    "            loss_ls = []\n",
    "            # print('iter >>', count, ',loss_ave >>', loss_ave, 'accuracy >>', acc)\n",
    "        else:\n",
    "            mb_ind = train_ind[mbi: mbi+mb]\n",
    "            mbi += mb\n",
    "\n",
    "        x = xs[mb_ind]\n",
    "        t = ts[mb_ind]\n",
    "\n",
    "        loss, acc = model.train_on_batch(x=x, y=t)\n",
    "        print('iter >>', count, ',loss >>', loss, 'accuracy >>', acc)\n",
    "        \n",
    "        loss_ls.append(loss)\n",
    "        count += 1\n",
    "\n",
    "        if loss_AveCnt == 50: break\n",
    "\n",
    "    model.save('LeNet.h5')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " ,loss >> 2.2524884 accuracy >> 0.35\niter >> 362 ,loss >> 1.8011628 accuracy >> 0.36\niter >> 363 ,loss >> 1.9804626 accuracy >> 0.34\niter >> 364 ,loss >> 2.2828422 accuracy >> 0.32\niter >> 365 ,loss >> 2.0984051 accuracy >> 0.38\niter >> 366 ,loss >> 1.8410748 accuracy >> 0.32\niter >> 367 ,loss >> 2.0234895 accuracy >> 0.36\niter >> 368 ,loss >> 1.9331087 accuracy >> 0.37\niter >> 369 ,loss >> 2.8728411 accuracy >> 0.31\niter >> 370 ,loss >> 2.5228205 accuracy >> 0.28\niter >> 371 ,loss >> 1.9322217 accuracy >> 0.34\niter >> 372 ,loss >> 2.7281756 accuracy >> 0.31\niter >> 373 ,loss >> 2.3368287 accuracy >> 0.43\niter >> 374 ,loss >> 2.181413 accuracy >> 0.31\niter >> 375 ,loss >> 2.860215 accuracy >> 0.29\niter >> 376 ,loss >> 2.2072654 accuracy >> 0.44\niter >> 377 ,loss >> 2.280387 accuracy >> 0.42\niter >> 378 ,loss >> 1.9181503 accuracy >> 0.3\niter >> 379 ,loss >> 2.183417 accuracy >> 0.31\niter >> 380 ,loss >> 1.6529671 accuracy >> 0.38\niter >> 381 ,loss >> 1.9956174 accuracy >> 0.4\niter >> 382 ,loss >> 1.9250388 accuracy >> 0.47\niter >> 383 ,loss >> 2.2956247 accuracy >> 0.34\niter >> 384 ,loss >> 1.9031799 accuracy >> 0.39\niter >> 385 ,loss >> 2.3396115 accuracy >> 0.38\niter >> 386 ,loss >> 2.1419294 accuracy >> 0.39\niter >> 387 ,loss >> 2.1203418 accuracy >> 0.36\niter >> 388 ,loss >> 2.233837 accuracy >> 0.29\niter >> 389 ,loss >> 3.840793 accuracy >> 0.29\niter >> 390 ,loss >> 2.914345 accuracy >> 0.31\niter >> 391 ,loss >> 4.477769 accuracy >> 0.28\niter >> 392 ,loss >> 4.118797 accuracy >> 0.31\niter >> 393 ,loss >> 3.5838957 accuracy >> 0.44\niter >> 394 ,loss >> 2.7577035 accuracy >> 0.33\niter >> 395 ,loss >> 2.3874645 accuracy >> 0.38\niter >> 396 ,loss >> 2.9642084 accuracy >> 0.34\niter >> 397 ,loss >> 3.4750893 accuracy >> 0.36\niter >> 398 ,loss >> 3.0005884 accuracy >> 0.32\niter >> 399 ,loss >> 2.2309403 accuracy >> 0.44\niter >> 400 ,loss >> 2.8162396 accuracy >> 0.34\niter >> 401 ,loss >> 2.1362152 accuracy >> 0.49\niter >> 402 ,loss >> 2.008151 accuracy >> 0.36\niter >> 403 ,loss >> 2.0972118 accuracy >> 0.47\niter >> 404 ,loss >> 1.5110612 accuracy >> 0.44\niter >> 405 ,loss >> 1.7248025 accuracy >> 0.42\niter >> 406 ,loss >> 2.2166326 accuracy >> 0.47\niter >> 407 ,loss >> 2.2648141 accuracy >> 0.45\niter >> 408 ,loss >> 2.6279404 accuracy >> 0.42\niter >> 409 ,loss >> 1.8813251 accuracy >> 0.45\niter >> 410 ,loss >> 2.6296062 accuracy >> 0.36\niter >> 411 ,loss >> 2.9011536 accuracy >> 0.3\niter >> 412 ,loss >> 3.3512115 accuracy >> 0.26\niter >> 413 ,loss >> 3.0806623 accuracy >> 0.34\niter >> 414 ,loss >> 2.5205572 accuracy >> 0.38\niter >> 415 ,loss >> 2.6977453 accuracy >> 0.4\niter >> 416 ,loss >> 2.5725262 accuracy >> 0.34\niter >> 417 ,loss >> 1.5641147 accuracy >> 0.44\niter >> 418 ,loss >> 1.9099784 accuracy >> 0.44\niter >> 419 ,loss >> 1.9720812 accuracy >> 0.49\niter >> 420 ,loss >> 2.969826 accuracy >> 0.31\niter >> 421 ,loss >> 3.4904468 accuracy >> 0.33\niter >> 422 ,loss >> 2.93526 accuracy >> 0.39\niter >> 423 ,loss >> 4.9168925 accuracy >> 0.36\niter >> 424 ,loss >> 3.2156205 accuracy >> 0.4\niter >> 425 ,loss >> 3.961076 accuracy >> 0.33\niter >> 426 ,loss >> 4.2857733 accuracy >> 0.33\niter >> 427 ,loss >> 2.2551124 accuracy >> 0.4\niter >> 428 ,loss >> 2.7907891 accuracy >> 0.43\niter >> 429 ,loss >> 4.166033 accuracy >> 0.36\niter >> 430 ,loss >> 4.8821926 accuracy >> 0.28\niter >> 431 ,loss >> 8.275848 accuracy >> 0.32\niter >> 432 ,loss >> 3.7596052 accuracy >> 0.38\niter >> 433 ,loss >> 2.893991 accuracy >> 0.38\niter >> 434 ,loss >> 4.907505 accuracy >> 0.39\niter >> 435 ,loss >> 4.35452 accuracy >> 0.35\niter >> 436 ,loss >> 6.6818953 accuracy >> 0.34\niter >> 437 ,loss >> 5.4006066 accuracy >> 0.37\niter >> 438 ,loss >> 5.812361 accuracy >> 0.38\niter >> 439 ,loss >> 5.120857 accuracy >> 0.29\niter >> 440 ,loss >> 2.7229676 accuracy >> 0.49\niter >> 441 ,loss >> 2.9721956 accuracy >> 0.39\niter >> 442 ,loss >> 2.1272497 accuracy >> 0.4\niter >> 443 ,loss >> 2.654734 accuracy >> 0.41\niter >> 444 ,loss >> 2.4459336 accuracy >> 0.4\niter >> 445 ,loss >> 2.076167 accuracy >> 0.42\niter >> 446 ,loss >> 2.322594 accuracy >> 0.42\niter >> 447 ,loss >> 3.2770007 accuracy >> 0.51\niter >> 448 ,loss >> 2.4254417 accuracy >> 0.42\niter >> 449 ,loss >> 7.051627 accuracy >> 0.39\niter >> 450 ,loss >> 5.6406612 accuracy >> 0.33\niter >> 451 ,loss >> 5.5808825 accuracy >> 0.4\niter >> 452 ,loss >> 7.925592 accuracy >> 0.37\niter >> 453 ,loss >> 4.432026 accuracy >> 0.41\niter >> 454 ,loss >> 2.9653919 accuracy >> 0.39\niter >> 455 ,loss >> 4.6456423 accuracy >> 0.36\niter >> 456 ,loss >> 3.8358767 accuracy >> 0.41\niter >> 457 ,loss >> 3.0979834 accuracy >> 0.5\niter >> 458 ,loss >> 5.1102824 accuracy >> 0.32\niter >> 459 ,loss >> 4.6090846 accuracy >> 0.42\niter >> 460 ,loss >> 2.9830005 accuracy >> 0.43\niter >> 461 ,loss >> 2.319581 accuracy >> 0.42\niter >> 462 ,loss >> 3.2798731 accuracy >> 0.47\niter >> 463 ,loss >> 3.2663476 accuracy >> 0.46\niter >> 464 ,loss >> 1.8801415 accuracy >> 0.48\niter >> 465 ,loss >> 2.8155913 accuracy >> 0.52\niter >> 466 ,loss >> 3.2920606 accuracy >> 0.34\niter >> 467 ,loss >> 4.2743697 accuracy >> 0.45\niter >> 468 ,loss >> 5.102375 accuracy >> 0.46\niter >> 469 ,loss >> 3.5115345 accuracy >> 0.38\niter >> 470 ,loss >> 8.365953 accuracy >> 0.37\niter >> 471 ,loss >> 6.2616796 accuracy >> 0.34\niter >> 472 ,loss >> 5.614557 accuracy >> 0.37\niter >> 473 ,loss >> 5.5524416 accuracy >> 0.34\niter >> 474 ,loss >> 6.6293645 accuracy >> 0.35\niter >> 475 ,loss >> 5.7290087 accuracy >> 0.31\niter >> 476 ,loss >> 4.0861893 accuracy >> 0.39\niter >> 477 ,loss >> 4.358242 accuracy >> 0.34\niter >> 478 ,loss >> 6.044554 accuracy >> 0.35\niter >> 479 ,loss >> 4.510336 accuracy >> 0.37\niter >> 480 ,loss >> 2.8614025 accuracy >> 0.39\niter >> 481 ,loss >> 2.6625729 accuracy >> 0.36\niter >> 482 ,loss >> 2.5558147 accuracy >> 0.4\niter >> 483 ,loss >> 4.35194 accuracy >> 0.42\niter >> 484 ,loss >> 4.607012 accuracy >> 0.33\niter >> 485 ,loss >> 2.781112 accuracy >> 0.49\niter >> 486 ,loss >> 2.9656794 accuracy >> 0.48\niter >> 487 ,loss >> 3.0953543 accuracy >> 0.45\niter >> 488 ,loss >> 4.8115215 accuracy >> 0.47\niter >> 489 ,loss >> 4.0611253 accuracy >> 0.35\niter >> 490 ,loss >> 4.5140386 accuracy >> 0.48\niter >> 491 ,loss >> 4.578175 accuracy >> 0.36\niter >> 492 ,loss >> 5.492571 accuracy >> 0.38\niter >> 493 ,loss >> 8.793851 accuracy >> 0.34\niter >> 494 ,loss >> 12.485647 accuracy >> 0.25\niter >> 495 ,loss >> 9.394262 accuracy >> 0.41\niter >> 496 ,loss >> 16.349997 accuracy >> 0.31\niter >> 497 ,loss >> 8.83135 accuracy >> 0.32\niter >> 498 ,loss >> 8.749364 accuracy >> 0.35\niter >> 499 ,loss >> 11.396926 accuracy >> 0.34\niter >> 500 ,loss >> 7.0863843 accuracy >> 0.39\niter >> 501 ,loss >> 4.5356054 accuracy >> 0.36\niter >> 502 ,loss >> 3.8069701 accuracy >> 0.5\niter >> 503 ,loss >> 3.8471656 accuracy >> 0.46\niter >> 504 ,loss >> 4.378333 accuracy >> 0.37\niter >> 505 ,loss >> 2.6482053 accuracy >> 0.4\niter >> 506 ,loss >> 3.6575463 accuracy >> 0.39\niter >> 507 ,loss >> 3.351836 accuracy >> 0.39\niter >> 508 ,loss >> 2.869961 accuracy >> 0.34\niter >> 509 ,loss >> 3.6867785 accuracy >> 0.31\niter >> 510 ,loss >> 3.4403002 accuracy >> 0.41\niter >> 511 ,loss >> 2.9402637 accuracy >> 0.51\niter >> 512 ,loss >> 3.5080023 accuracy >> 0.42\niter >> 513 ,loss >> 6.450154 accuracy >> 0.42\niter >> 514 ,loss >> 4.0969934 accuracy >> 0.44\niter >> 515 ,loss >> 2.9695902 accuracy >> 0.49\niter >> 516 ,loss >> 4.944589 accuracy >> 0.38\niter >> 517 ,loss >> 2.4306912 accuracy >> 0.45\niter >> 518 ,loss >> 4.599767 accuracy >> 0.46\niter >> 519 ,loss >> 4.3135257 accuracy >> 0.42\niter >> 520 ,loss >> 7.8925185 accuracy >> 0.38\niter >> 521 ,loss >> 13.310461 accuracy >> 0.34\niter >> 522 ,loss >> 7.4096007 accuracy >> 0.43\niter >> 523 ,loss >> 6.1831374 accuracy >> 0.44\niter >> 524 ,loss >> 11.839791 accuracy >> 0.43\niter >> 525 ,loss >> 4.9867253 accuracy >> 0.43\niter >> 526 ,loss >> 12.146875 accuracy >> 0.36\niter >> 527 ,loss >> 5.53895 accuracy >> 0.33\niter >> 528 ,loss >> 15.613276 accuracy >> 0.39\niter >> 529 ,loss >> 6.2820644 accuracy >> 0.37\niter >> 530 ,loss >> 5.859805 accuracy >> 0.35\niter >> 531 ,loss >> 21.761877 accuracy >> 0.29\niter >> 532 ,loss >> 10.515107 accuracy >> 0.35\niter >> 533 ,loss >> 16.415325 accuracy >> 0.22\niter >> 534 ,loss >> 9.086078 accuracy >> 0.4\niter >> 535 ,loss >> 12.037895 accuracy >> 0.3\niter >> 536 ,loss >> 8.273247 accuracy >> 0.33\niter >> 537 ,loss >> 5.8360763 accuracy >> 0.37\niter >> 538 ,loss >> 11.279711 accuracy >> 0.37\niter >> 539 ,loss >> 9.746688 accuracy >> 0.36\niter >> 540 ,loss >> 4.5416903 accuracy >> 0.41\niter >> 541 ,loss >> 4.027169 accuracy >> 0.39\niter >> 542 ,loss >> 4.6043105 accuracy >> 0.48\niter >> 543 ,loss >> 5.7083726 accuracy >> 0.36\niter >> 544 ,loss >> 3.8271246 accuracy >> 0.44\niter >> 545 ,loss >> 3.4926624 accuracy >> 0.55\niter >> 546 ,loss >> 1.7077676 accuracy >> 0.51\niter >> 547 ,loss >> 2.2351606 accuracy >> 0.32\niter >> 548 ,loss >> 2.6787157 accuracy >> 0.51\niter >> 549 ,loss >> 2.4973497 accuracy >> 0.55\niter >> 550 ,loss >> 3.1612208 accuracy >> 0.48\niter >> 551 ,loss >> 3.1507068 accuracy >> 0.5\niter >> 552 ,loss >> 2.2222996 accuracy >> 0.54\niter >> 553 ,loss >> 3.5419092 accuracy >> 0.45\niter >> 554 ,loss >> 6.9006763 accuracy >> 0.41\niter >> 555 ,loss >> 13.685659 accuracy >> 0.34\niter >> 556 ,loss >> 15.270816 accuracy >> 0.25\niter >> 557 ,loss >> 31.977217 accuracy >> 0.2\niter >> 558 ,loss >> 25.050396 accuracy >> 0.21\niter >> 559 ,loss >> 17.140755 accuracy >> 0.18\niter >> 560 ,loss >> 26.703054 accuracy >> 0.22\niter >> 561 ,loss >> 26.6137 accuracy >> 0.25\niter >> 562 ,loss >> 26.174969 accuracy >> 0.11\niter >> 563 ,loss >> 10.011634 accuracy >> 0.14\niter >> 564 ,loss >> 5.608872 accuracy >> 0.08\niter >> 565 ,loss >> 3.9856625 accuracy >> 0.12\niter >> 566 ,loss >> 5.798836 accuracy >> 0.12\niter >> 567 ,loss >> 3.7319863 accuracy >> 0.11\niter >> 568 ,loss >> 4.004356 accuracy >> 0.11\niter >> 569 ,loss >> 3.3640664 accuracy >> 0.14\niter >> 570 ,loss >> 3.8096528 accuracy >> 0.17\niter >> 571 ,loss >> 3.3519514 accuracy >> 0.1\niter >> 572 ,loss >> 3.399159 accuracy >> 0.1\niter >> 573 ,loss >> 3.6593323 accuracy >> 0.02\niter >> 574 ,loss >> 3.4925709 accuracy >> 0.03\niter >> 575 ,loss >> 3.3207684 accuracy >> 0.05\niter >> 576 ,loss >> 3.452522 accuracy >> 0.05\niter >> 577 ,loss >> 3.4250195 accuracy >> 0.05\niter >> 578 ,loss >> 3.4860451 accuracy >> 0.01\niter >> 579 ,loss >> 3.5356972 accuracy >> 0.11\niter >> 580 ,loss >> 3.438351 accuracy >> 0.05\niter >> 581 ,loss >> 3.5418916 accuracy >> 0.05\niter >> 582 ,loss >> 3.3590357 accuracy >> 0.04\niter >> 583 ,loss >> 3.4460871 accuracy >> 0.1\niter >> 584 ,loss >> 3.3711627 accuracy >> 0.08\niter >> 585 ,loss >> 3.4225712 accuracy >> 0.06\niter >> 586 ,loss >> 3.4989853 accuracy >> 0.05\niter >> 587 ,loss >> 3.4350567 accuracy >> 0.05\niter >> 588 ,loss >> 3.4814837 accuracy >> 0.01\niter >> 589 ,loss >> 3.401483 accuracy >> 0.07\niter >> 590 ,loss >> 3.564875 accuracy >> 0.05\niter >> 591 ,loss >> 3.3209262 accuracy >> 0.09\niter >> 592 ,loss >> 3.429886 accuracy >> 0.07\niter >> 593 ,loss >> 3.4472723 accuracy >> 0.02\niter >> 594 ,loss >> 3.4304497 accuracy >> 0.04\niter >> 595 ,loss >> 3.4163485 accuracy >> 0.01\niter >> 596 ,loss >> 3.320629 accuracy >> 0.08\niter >> 597 ,loss >> 3.362805 accuracy >> 0.02\niter >> 598 ,loss >> 3.4458418 accuracy >> 0.04\niter >> 599 ,loss >> 3.2709272 accuracy >> 0.03\niter >> 600 ,loss >> 3.4417136 accuracy >> 0.06\niter >> 601 ,loss >> 3.2713294 accuracy >> 0.08\niter >> 602 ,loss >> 3.2968671 accuracy >> 0.04\niter >> 603 ,loss >> 3.3445544 accuracy >> 0.05\niter >> 604 ,loss >> 3.4012253 accuracy >> 0.09\niter >> 605 ,loss >> 3.35632 accuracy >> 0.11\niter >> 606 ,loss >> 3.3325455 accuracy >> 0.09\niter >> 607 ,loss >> 3.3592424 accuracy >> 0.1\niter >> 608 ,loss >> 3.3145897 accuracy >> 0.12\niter >> 609 ,loss >> 3.2574432 accuracy >> 0.1\niter >> 610 ,loss >> 3.2866113 accuracy >> 0.11\niter >> 611 ,loss >> 3.1747286 accuracy >> 0.14\niter >> 612 ,loss >> 3.1524541 accuracy >> 0.08\niter >> 613 ,loss >> 3.2301202 accuracy >> 0.15\niter >> 614 ,loss >> 3.2355149 accuracy >> 0.15\niter >> 615 ,loss >> 3.2779937 accuracy >> 0.06\niter >> 616 ,loss >> 3.6190777 accuracy >> 0.06\niter >> 617 ,loss >> 3.5569985 accuracy >> 0.05\niter >> 618 ,loss >> 3.3400521 accuracy >> 0.06\niter >> 619 ,loss >> 3.2569196 accuracy >> 0.11\niter >> 620 ,loss >> 3.236947 accuracy >> 0.11\niter >> 621 ,loss >> 3.2251441 accuracy >> 0.13\niter >> 622 ,loss >> 3.1366765 accuracy >> 0.1\niter >> 623 ,loss >> 3.3948505 accuracy >> 0.09\niter >> 624 ,loss >> 3.2217164 accuracy >> 0.05\niter >> 625 ,loss >> 3.1240602 accuracy >> 0.09\niter >> 626 ,loss >> 3.2443306 accuracy >> 0.08\niter >> 627 ,loss >> 3.1450117 accuracy >> 0.15\niter >> 628 ,loss >> 3.0817187 accuracy >> 0.14\niter >> 629 ,loss >> 3.1390443 accuracy >> 0.14\niter >> 630 ,loss >> 3.2150934 accuracy >> 0.06\niter >> 631 ,loss >> 3.065187 accuracy >> 0.13\niter >> 632 ,loss >> 2.9710696 accuracy >> 0.21\niter >> 633 ,loss >> 3.1705134 accuracy >> 0.12\niter >> 634 ,loss >> 2.9003336 accuracy >> 0.14\niter >> 635 ,loss >> 2.9486816 accuracy >> 0.14\niter >> 636 ,loss >> 3.1818118 accuracy >> 0.12\niter >> 637 ,loss >> 3.188729 accuracy >> 0.09\niter >> 638 ,loss >> 5.4088287 accuracy >> 0.04\niter >> 639 ,loss >> 3.3326905 accuracy >> 0.07\niter >> 640 ,loss >> 3.2950904 accuracy >> 0.05\niter >> 641 ,loss >> 3.3958993 accuracy >> 0.05\niter >> 642 ,loss >> 3.3493078 accuracy >> 0.08\niter >> 643 ,loss >> 3.3726134 accuracy >> 0.05\niter >> 644 ,loss >> 3.2323797 accuracy >> 0.06\niter >> 645 ,loss >> 3.5274246 accuracy >> 0.04\niter >> 646 ,loss >> 3.4213505 accuracy >> 0.06\niter >> 647 ,loss >> 3.4271686 accuracy >> 0.1\niter >> 648 ,loss >> 3.4312196 accuracy >> 0.06\niter >> 649 ,loss >> 3.476737 accuracy >> 0.06\niter >> 650 ,loss >> 3.3755145 accuracy >> 0.1\niter >> 651 ,loss >> 3.37325 accuracy >> 0.05\niter >> 652 ,loss >> 3.4064398 accuracy >> 0.09\niter >> 653 ,loss >> 3.376372 accuracy >> 0.06\niter >> 654 ,loss >> 3.3884864 accuracy >> 0.09\niter >> 655 ,loss >> 3.2444327 accuracy >> 0.1\niter >> 656 ,loss >> 3.2742317 accuracy >> 0.11\niter >> 657 ,loss >> 3.4158347 accuracy >> 0.12\niter >> 658 ,loss >> 3.301073 accuracy >> 0.06\niter >> 659 ,loss >> 3.3786402 accuracy >> 0.09\niter >> 660 ,loss >> 3.1712658 accuracy >> 0.12\niter >> 661 ,loss >> 3.3503072 accuracy >> 0.11\niter >> 662 ,loss >> 3.3512769 accuracy >> 0.19\niter >> 663 ,loss >> 3.2993069 accuracy >> 0.09\niter >> 664 ,loss >> 3.230074 accuracy >> 0.12\niter >> 665 ,loss >> 3.33902 accuracy >> 0.13\niter >> 666 ,loss >> 2.9295897 accuracy >> 0.17\niter >> 667 ,loss >> 3.0966678 accuracy >> 0.19\niter >> 668 ,loss >> 3.0708327 accuracy >> 0.14\niter >> 669 ,loss >> 3.3602374 accuracy >> 0.18\niter >> 670 ,loss >> 2.889591 accuracy >> 0.18\niter >> 671 ,loss >> 2.9494433 accuracy >> 0.21\niter >> 672 ,loss >> 2.9454944 accuracy >> 0.17\niter >> 673 ,loss >> 3.0091085 accuracy >> 0.15\niter >> 674 ,loss >> 3.1204572 accuracy >> 0.1\niter >> 675 ,loss >> 3.223869 accuracy >> 0.17\niter >> 676 ,loss >> 2.8212295 accuracy >> 0.19\niter >> 677 ,loss >> 2.8968005 accuracy >> 0.2\niter >>678 ,loss >> 2.7993402 accuracy >> 0.23\niter >> 679 ,loss >> 2.8378997 accuracy >> 0.21\niter >> 680 ,loss >> 2.4806821 accuracy >> 0.25\niter >> 681 ,loss >> 2.9091837 accuracy >> 0.25\niter >> 682 ,loss >> 2.7955275 accuracy >> 0.14\niter >> 683 ,loss >> 3.073487 accuracy >> 0.2\niter >> 684 ,loss >> 2.760338 accuracy >> 0.26\niter >> 685 ,loss >> 2.8614554 accuracy >> 0.26\niter >> 686 ,loss >> 2.5031824 accuracy >> 0.33\niter >> 687 ,loss >> 2.6928968 accuracy >> 0.22\niter >> 688 ,loss >> 2.5242872 accuracy >> 0.33\niter >> 689 ,loss >> 2.2131536 accuracy >> 0.39\niter >> 690 ,loss >> 2.6279986 accuracy >> 0.31\niter >> 691 ,loss >> 2.6468678 accuracy >> 0.31\niter >> 692 ,loss >> 2.5421207 accuracy >> 0.28\niter >> 693 ,loss >> 2.763843 accuracy >> 0.09\niter >> 694 ,loss >> 2.3796265 accuracy >> 0.34\niter >> 695 ,loss >> 2.3473628 accuracy >> 0.35\niter >> 696 ,loss >> 2.237855 accuracy >> 0.28\niter >> 697 ,loss >> 2.4221435 accuracy >> 0.3\niter >> 698 ,loss >> 2.1845832 accuracy >> 0.41\niter >> 699 ,loss >> 2.3135629 accuracy >> 0.34\niter >> 700 ,loss >> 2.2582693 accuracy >> 0.38\niter >> 701 ,loss >> 2.160737 accuracy >> 0.34\niter >> 702 ,loss >> 2.510029 accuracy >> 0.36\niter >> 703 ,loss >> 2.0732906 accuracy >> 0.31\niter >> 704 ,loss >> 1.9374988 accuracy >> 0.46\niter >> 705 ,loss >> 2.5881586 accuracy >> 0.4\niter >> 706 ,loss >> 2.1282074 accuracy >> 0.36\niter >> 707 ,loss >> 2.375785 accuracy >> 0.39\niter >> 708 ,loss >> 2.3012147 accuracy >> 0.29\niter >> 709 ,loss >> 2.0392714 accuracy >> 0.32\niter >> 710 ,loss >> 2.3604019 accuracy >> 0.38\niter >> 711 ,loss >> 2.1915238 accuracy >> 0.32\niter >> 712 ,loss >> 1.8632114 accuracy >> 0.39\niter >> 713 ,loss >> 2.7059171 accuracy >> 0.31\niter >> 714 ,loss >> 1.8286514 accuracy >> 0.45\niter >> 715 ,loss >> 2.2181716 accuracy >> 0.36\niter >> 716 ,loss >> 2.1228714 accuracy >> 0.29\niter >> 717 ,loss >> 1.8135971 accuracy >> 0.34\niter >> 718 ,loss >> 1.8533702 accuracy >> 0.35\niter >> 719 ,loss >> 1.8783351 accuracy >> 0.41\niter >> 720 ,loss >> 1.8408355 accuracy >> 0.44\niter >> 721 ,loss >> 2.0148294 accuracy >> 0.45\niter >> 722 ,loss >> 1.9199084 accuracy >> 0.36\niter >> 723 ,loss >> 1.8694558 accuracy >> 0.43\niter >> 724 ,loss >> 2.059095 accuracy >> 0.4\niter >> 725 ,loss >> 2.0154345 accuracy >> 0.38\niter >> 726 ,loss >> 1.9906075 accuracy >> 0.43\niter >> 727 ,loss >> 1.8535323 accuracy >> 0.44\niter >> 728 ,loss >> 1.8720047 accuracy >> 0.42\niter >> 729 ,loss >> 1.7818927 accuracy >> 0.37\niter >> 730 ,loss >> 1.9568628 accuracy >> 0.46\niter >> 731 ,loss >> 2.2173347 accuracy >> 0.29\niter >> 732 ,loss >> 1.9849274 accuracy >> 0.38\niter >> 733 ,loss >> 1.7061641 accuracy >> 0.37\niter >> 734 ,loss >> 1.9969977 accuracy >> 0.52\niter >> 735 ,loss >> 1.9001635 accuracy >> 0.37\niter >> 736 ,loss >> 1.8148612 accuracy >> 0.36\niter >> 737 ,loss >> 2.1410294 accuracy >> 0.42\niter >> 738 ,loss >> 2.1436985 accuracy >> 0.42\niter >> 739 ,loss >> 2.168243 accuracy >> 0.31\niter >> 740 ,loss >> 2.015415 accuracy >> 0.4\niter >> 741 ,loss >> 2.1603634 accuracy >> 0.32\niter >> 742 ,loss >> 2.0453637 accuracy >> 0.35\niter >> 743 ,loss >> 2.0985434 accuracy >> 0.34\niter >> 744 ,loss >> 1.8921887 accuracy >> 0.32\niter >> 745 ,loss >> 2.0745497 accuracy >> 0.37\niter >> 746 ,loss >> 1.7460672 accuracy >> 0.34\niter >> 747 ,loss >> 2.352457 accuracy >> 0.26\niter >> 748 ,loss >> 2.0179415 accuracy >> 0.29\niter >> 749 ,loss >> 3.7035804 accuracy >> 0.13\niter >> 750 ,loss >> 4.1532426 accuracy >> 0.12\niter >> 751 ,loss >> 3.3922613 accuracy >> 0.3\niter >> 752 ,loss >> 3.748634 accuracy >> 0.23\niter >> 753 ,loss >> 4.2353272 accuracy >> 0.29\niter >> 754 ,loss >> 4.7562 accuracy >> 0.25\niter >> 755 ,loss >> 4.860667 accuracy >> 0.25\niter >> 756 ,loss >> 4.205384 accuracy >> 0.23\niter >> 757 ,loss >> 4.514531 accuracy >> 0.27\niter >> 758 ,loss >> 5.308868 accuracy >> 0.31\niter >> 759 ,loss >> 10.591479 accuracy >> 0.17\niter >> 760 ,loss >> 8.527083 accuracy >> 0.19\niter >> 761 ,loss >> 7.0027776 accuracy >> 0.18\niter >> 762 ,loss >> 13.335955 accuracy >> 0.11\niter >> 763 ,loss >> 10.802825 accuracy >> 0.12\niter >> 764 ,loss >> 8.730687 accuracy >> 0.17\niter >> 765 ,loss >> 9.368692 accuracy >> 0.13\niter >> 766 ,loss >> 4.7393074 accuracy >> 0.13\niter >> 767 ,loss >> 3.846034 accuracy >> 0.11\niter >> 768 ,loss >> 4.7821765 accuracy >> 0.09\niter >> 769 ,loss >> 4.86073 accuracy >> 0.04\niter >> 770 ,loss >> 3.9666803 accuracy >> 0.07\niter >> 771 ,loss >> 3.7111475 accuracy >> 0.07\niter >> 772 ,loss >> 3.4513583 accuracy >> 0.08\niter >> 773 ,loss >> 3.6519897 accuracy >> 0.08\niter >> 774 ,loss >> 3.5286095 accuracy >> 0.07\niter >> 775 ,loss >> 3.3660004 accuracy >> 0.11\niter >> 776 ,loss >> 3.4891214 accuracy >> 0.03\niter >> 777 ,loss >> 3.612032 accuracy >> 0.06\niter >> 778 ,loss >> 3.2441854 accuracy >> 0.14\niter >> 779 ,loss >> 3.289994 accuracy >> 0.15\niter >> 780 ,loss >> 3.2058153 accuracy >> 0.13\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-508963e9f8e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#train(DirPath, img_size=(64, 64), cls_label=CLS)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDirPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCLS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-dda9e8315a46>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(DirPath, img_size, cls_label)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmb_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iter >>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m',loss >>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy >>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# データを読み込むフォルダを指定する\n",
    "DirPath = '../../../DataSet/AngleDetection/training/*'\n",
    "#print(DirPath)\n",
    "\n",
    "num_classes = 36\n",
    "img_width, img_height = 64, 64\n",
    "CLS = np.arange(0, 175, 5).astype('str')\n",
    "\n",
    "#train(DirPath, img_size=(64, 64), cls_label=CLS)\n",
    "train(DirPath, img_size=(img_width, img_height), cls_label=CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirPath = '../../../DataSet/AngleDetection/training/*'\n",
    "img_size = (64, 64)\n",
    "CLS = np.arange(0, 175, 5).astype('str')\n",
    "\n",
    "xs, ts, paths, _ = data_load(DirPath, img_size[0], img_size[1], CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(30694, 64, 64, 3)\n(30694, 36)\n(100, 64, 64, 3)\n(100, 36)\n"
    }
   ],
   "source": [
    "print(xs.shape)\n",
    "print(ts.shape)\n",
    "\n",
    "mb = 100\n",
    "mbi = 0\n",
    "\n",
    "train_ind = np.arange(len(xs))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(train_ind)\n",
    "\n",
    "mb_ind = train_ind[mbi: mbi+mb]\n",
    "mbi += mb\n",
    "\n",
    "x = xs[mb_ind]\n",
    "t = ts[mb_ind]\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}