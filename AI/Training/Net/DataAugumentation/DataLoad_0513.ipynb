{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'d:\\\\My_programing\\\\python\\\\AI\\\\Training\\\\Net\\\\DataAugumentation'"
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# 相対パスを書くために、現在のディレクトリの位置を書き出す\n",
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def data_load(path, img_width, img_height, CLS):\n",
    "    \"\"\"ファイルパスからデータセットをロードするための関数\"\"\"\n",
    "    xs = [] # 訓練データ用の空リスト\n",
    "    ts = [] # データラベル用の空リスト\n",
    "    paths = [] # ロードしたファイルパス用の空リスト\n",
    "    img_read_err = [] # ロードできなかったファイルパス用の空リスト\n",
    "\n",
    "    # データをロードするためにディレクトリを下りていく\n",
    "    for dir_path_1 in glob(DirPath):\n",
    "        for dir_path_2 in glob(dir_path_1 + '/*'):\n",
    "            for path in glob(dir_path_2 + '/*'):\n",
    "                #print(path, 'を読み込みました。')\n",
    "\n",
    "                # 訓練用画像を読み込む\n",
    "                x = cv2.imread(path)\n",
    "                if x is None: # もし，画像がロードできなかった場合\n",
    "                    print(path, 'を読み込めませんでした．')\n",
    "                    img_read_err.append(path)\n",
    "                    continue\n",
    "                else: # 画像がロードできた場合\n",
    "                    x = cv2.resize(x, (img_width, img_height)).astype(np.float32)\n",
    "                    x /= 255.\n",
    "                    xs.append(x)\n",
    "                \n",
    "                # one-hot-labelを作成する\n",
    "                t = np.zeros(num_classes)\n",
    "                p = dir_path_1[dir_path_1.find('\\\\'):].strip('\\\\')\n",
    "                for i, cls in enumerate(CLS):\n",
    "                    if cls == p:\n",
    "                        t[i] = 1\n",
    "\n",
    "                ts.append(t)\n",
    "                paths.append(path)\n",
    "\n",
    "    xs = np.array(xs, dtype=np.float32)\n",
    "    ts = np.array(ts, dtype=np.int)\n",
    "\n",
    "    return xs, ts, paths, img_read_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNetを作成する\n",
    "## Network\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, Input, BatchNormalization\n",
    "\n",
    "def LeNet(img_width, img_height, num_classes):\n",
    "    inputs = Input((img_height, img_width, 3))\n",
    "    x = Conv2D(6, (5, 5), padding='valid', activation=None, name='conv1')(inputs)\n",
    "    x = MaxPool2D((2, 2), padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    x = Conv2D(16, (5, 5), padding='valid', activation=None, name='conv2')(x)\n",
    "    x = MaxPool2D((2, 2), padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(120, name='dense1', activation=None)(x)\n",
    "    x = Dense(64, name='dense2', activation=None)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name='model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(DirPath, img_size, cls_label):\n",
    "    model = LeNet(img_size[0], img_size[1], len(cls_label)+1)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    xs, ts, paths, _ = data_load(DirPath, img_size[0], img_size[1], cls_label)\n",
    "\n",
    "    # training\n",
    "    mb = 8\n",
    "    mbi = 0\n",
    "    train_ind = np.arange(len(xs))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(train_ind)\n",
    "\n",
    "    for i in range(500):\n",
    "        if mbi + mb > len(xs):\n",
    "            mb_ind = train_ind[mbi:]\n",
    "            np.random.shuffule(train_ind)\n",
    "            mb_ind = np.hstack((mb_ind, train_ind[:(mb-(len(xs)-mbi))]))\n",
    "            mbi = mb - (len(xs) - mbi)\n",
    "        else:\n",
    "            mb_ind = train_ind[mbi: mbi+mb]\n",
    "            mbi += mb\n",
    "\n",
    "        x = xs[mb_ind]\n",
    "        t = ts[mb_ind]\n",
    "\n",
    "        loss, acc = model.train_on_batch(x=x, y=t)\n",
    "        print('iter >>', i+1, ',loss >>', loss, 'accuracy >>', acc)\n",
    "\n",
    "    model.save('LeNet.h5')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y >> 0.125\niter >> 77 ,loss >> 3.120249 accuracy >> 0.0\niter >> 78 ,loss >> 3.1556344 accuracy >> 0.0\niter >> 79 ,loss >> 3.5754528 accuracy >> 0.125\niter >> 80 ,loss >> 3.62537 accuracy >> 0.0\niter >> 81 ,loss >> 3.6660626 accuracy >> 0.0\niter >> 82 ,loss >> 3.5605164 accuracy >> 0.0\niter >> 83 ,loss >> 3.6067295 accuracy >> 0.0\niter >> 84 ,loss >> 3.1216013 accuracy >> 0.0\niter >> 85 ,loss >> 3.618331 accuracy >> 0.0\niter >> 86 ,loss >> 3.5423725 accuracy >> 0.0\niter >> 87 ,loss >> 3.557204 accuracy >> 0.0\niter >> 88 ,loss >> 3.55056 accuracy >> 0.0\niter >> 89 ,loss >> 3.055048 accuracy >> 0.0\niter >> 90 ,loss >> 3.6598592 accuracy >> 0.0\niter >> 91 ,loss >> 3.528348 accuracy >> 0.125\niter >> 92 ,loss >> 3.6100626 accuracy >> 0.0\niter >> 93 ,loss >> 3.6029131 accuracy >> 0.0\niter >> 94 ,loss >> 3.5489721 accuracy >> 0.125\niter >> 95 ,loss >> 3.604447 accuracy >> 0.0\niter >> 96 ,loss >> 3.1702156 accuracy >> 0.0\niter >> 97 ,loss >> 3.552099 accuracy >> 0.0\niter >> 98 ,loss >> 3.5034196 accuracy >> 0.0\niter >> 99 ,loss >> 3.5670447 accuracy >> 0.125\niter >> 100 ,loss >> 3.5678453 accuracy >> 0.125\niter >> 101 ,loss >> 3.6266942 accuracy >> 0.0\niter >> 102 ,loss >> 3.5975916 accuracy >> 0.0\niter >> 103 ,loss >> 3.588696 accuracy >> 0.0\niter >> 104 ,loss >> 3.6193933 accuracy >> 0.0\niter >> 105 ,loss >> 3.6181922 accuracy >> 0.0\niter >> 106 ,loss >> 3.6042864 accuracy >> 0.0\niter >> 107 ,loss >> 3.0886242 accuracy >> 0.125\niter >> 108 ,loss >> 3.1150744 accuracy >> 0.0\niter >> 109 ,loss >> 3.5782247 accuracy >> 0.0\niter >> 110 ,loss >> 3.5309145 accuracy >> 0.125\niter >> 111 ,loss >> 3.6392043 accuracy >> 0.0\niter >> 112 ,loss >> 3.511912 accuracy >> 0.125\niter >> 113 ,loss >> 3.5599554 accuracy >> 0.0\niter >> 114 ,loss >> 3.507204 accuracy >> 0.25\niter >> 115 ,loss >> 3.0915778 accuracy >> 0.125\niter >> 116 ,loss >> 3.1281965 accuracy >> 0.125\niter >> 117 ,loss >> 3.5197692 accuracy >> 0.125\niter >> 118 ,loss >> 3.60199 accuracy >> 0.0\niter >> 119 ,loss >> 3.5339763 accuracy >> 0.125\niter >> 120 ,loss >> 3.5448637 accuracy >> 0.0\niter >> 121 ,loss >> 3.6320765 accuracy >> 0.0\niter >> 122 ,loss >> 3.116914 accuracy >> 0.0\niter >> 123 ,loss >> 3.6627445 accuracy >> 0.0\niter >> 124 ,loss >> 3.6715355 accuracy >> 0.0\niter >> 125 ,loss >> 3.6035275 accuracy >> 0.0\niter >> 126 ,loss >> 3.5443797 accuracy >> 0.0\niter >> 127 ,loss >> 3.1018727 accuracy >> 0.0\niter >> 128 ,loss >> 3.5901828 accuracy >> 0.0\niter >> 129 ,loss >> 3.5585096 accuracy >> 0.0\niter >> 130 ,loss >> 3.5646052 accuracy >> 0.0\niter >> 131 ,loss >> 3.6220818 accuracy >> 0.0\niter >> 132 ,loss >> 3.1378727 accuracy >>0.0\niter >> 133 ,loss >> 3.5301847 accuracy >> 0.125\niter >> 134 ,loss >> 3.601651 accuracy >> 0.0\niter >> 135 ,loss >> 3.5590885 accuracy >> 0.125\niter >> 136 ,loss >> 3.056084 accuracy >> 0.0\niter >> 137 ,loss >> 3.0528293 accuracy >> 0.0\niter >> 138 ,loss >> 3.6561651 accuracy >> 0.0\niter >> 139 ,loss >> 3.6516967 accuracy >> 0.0\niter >> 140 ,loss >> 3.6151724 accuracy >> 0.0\niter >> 141 ,loss >> 3.554773 accuracy >> 0.0\niter >> 142 ,loss >> 3.6459439 accuracy >> 0.0\niter >> 143 ,loss >> 3.499403 accuracy >> 0.0\niter >> 144 ,loss >> 3.63755 accuracy >> 0.0\niter >> 145 ,loss >> 3.4843857 accuracy >> 0.125\niter >> 146 ,loss >> 3.6519754 accuracy >> 0.0\niter >> 147 ,loss >> 3.6368456 accuracy >> 0.125\niter >> 148 ,loss >> 3.5331125 accuracy >> 0.0\niter >> 149 ,loss >> 3.656858 accuracy >> 0.0\niter >> 150 ,loss >> 3.5464506 accuracy >> 0.0\niter >> 151 ,loss >> 3.5410416 accuracy >> 0.0\niter >> 152 ,loss >> 3.0706487 accuracy >> 0.0\niter >> 153 ,loss >> 3.5742066 accuracy >> 0.0\niter >> 154 ,loss >> 3.5707028 accuracy >> 0.0\niter >> 155 ,loss >> 3.2438946 accuracy >> 0.0\niter >> 156 ,loss >> 3.1214762 accuracy >> 0.0\niter >> 157 ,loss >> 2.6588008 accuracy >> 0.0\niter >> 158 ,loss >> 3.5681286 accuracy >> 0.0\niter >> 159 ,loss >> 3.505218 accuracy >> 0.0\niter >> 160 ,loss >> 3.6432135 accuracy >> 0.0\niter >> 161 ,loss >> 3.143081 accuracy >> 0.0\niter >> 162 ,loss >> 3.578353 accuracy >> 0.0\niter >> 163 ,loss >> 3.1830397 accuracy >> 0.0\niter >> 164 ,loss >> 3.5450087 accuracy >> 0.0\niter >> 165 ,loss >> 3.6416135 accuracy >> 0.0\niter >> 166 ,loss >> 3.5869586 accuracy >> 0.0\niter >> 167 ,loss >> 3.469926 accuracy >> 0.0\niter >> 168 ,loss >> 3.4568155 accuracy >> 0.25\niter >> 169 ,loss >> 3.5650225 accuracy >> 0.0\niter >> 170 ,loss >> 3.549412 accuracy >> 0.0\niter >> 171 ,loss >> 3.5633774 accuracy >> 0.0\niter >> 172 ,loss >> 3.657898 accuracy >> 0.0\niter >> 173 ,loss >> 3.454257 accuracy >> 0.0\niter >> 174 ,loss >> 3.6421025 accuracy >> 0.0\niter >> 175 ,loss >> 3.6457806 accuracy >> 0.0\niter >> 176 ,loss >> 3.620265 accuracy >> 0.0\niter >> 177 ,loss >> 3.199271 accuracy >> 0.0\niter >> 178 ,loss >> 3.5453513 accuracy >> 0.0\niter >> 179 ,loss >> 3.529325 accuracy >> 0.0\niter >> 180 ,loss >> 2.6391993 accuracy >> 0.0\niter >> 181 ,loss >> 3.1977978 accuracy >> 0.0\niter >> 182 ,loss >> 3.5740616 accuracy >> 0.0\niter >> 183 ,loss >> 3.6883864 accuracy >> 0.0\niter >> 184 ,loss >> 3.2058887 accuracy >> 0.0\niter >> 185 ,loss >> 3.5950992 accuracy >> 0.0\niter >> 186 ,loss >> 3.606877 accuracy >> 0.0\niter >> 187 ,loss >> 3.5781283 accuracy >> 0.0\niter >> 188 ,loss >> 3.5490017 accuracy >> 0.125\niter >> 189 ,loss >> 3.1767294 accuracy >> 0.0\niter >> 190 ,loss >> 3.7164135 accuracy >> 0.0\niter >> 191 ,loss >> 3.535752 accuracy >> 0.125\niter >> 192 ,loss >> 3.6030228 accuracy >> 0.0\niter >> 193 ,loss >> 3.4581685 accuracy >> 0.125\niter >> 194 ,loss >> 3.5923097 accuracy >> 0.0\niter >> 195 ,loss >> 3.596177 accuracy >> 0.0\niter >> 196 ,loss >> 3.1684535 accuracy >> 0.0\niter >> 197 ,loss >> 3.5952215 accuracy >> 0.0\niter >> 198 ,loss >> 3.5027246 accuracy >> 0.0\niter >> 199 ,loss >> 3.599819 accuracy >> 0.0\niter >> 200 ,loss >> 2.651908 accuracy >> 0.0\niter >> 201 ,loss >> 3.5972943 accuracy >> 0.0\niter >> 202 ,loss >> 3.63988 accuracy >> 0.0\niter >> 203 ,loss >> 3.5032759 accuracy >> 0.125\niter >> 204 ,loss >> 3.5682425 accuracy >> 0.0\niter >> 205 ,loss >> 3.6206226 accuracy >> 0.0\niter >> 206 ,loss >> 3.6616511 accuracy >> 0.0\niter >> 207 ,loss >> 3.5326676 accuracy >> 0.0\niter >> 208 ,loss >> 3.556469 accuracy >> 0.0\niter >> 209 ,loss >> 3.1080024 accuracy >> 0.0\niter >> 210 ,loss >> 3.6481156 accuracy >> 0.0\niter >> 211 ,loss >> 3.576865 accuracy >> 0.0\niter >> 212 ,loss >> 3.1334794 accuracy >> 0.125\niter >> 213 ,loss >> 3.054174 accuracy >> 0.0\niter >> 214 ,loss >> 3.111534 accuracy >> 0.0\niter >> 215 ,loss >> 3.0999188 accuracy >> 0.125\niter >> 216 ,loss >> 3.6482337 accuracy >> 0.0\niter >> 217 ,loss >> 3.1044836 accuracy >> 0.0\niter >> 218 ,loss >> 3.1312447 accuracy >> 0.0\niter >> 219 ,loss >> 3.5193543 accuracy >> 0.0\niter >> 220 ,loss >> 3.582363 accuracy >> 0.0\niter >> 221 ,loss >> 3.5862365 accuracy >> 0.0\niter >> 222 ,loss >> 3.1883779 accuracy >> 0.0\niter >> 223 ,loss >> 3.586241 accuracy >> 0.0\niter >> 224 ,loss >> 3.1064582 accuracy >> 0.0\niter >> 225 ,loss >> 3.63078 accuracy >> 0.125\niter >> 226 ,loss >> 3.5906897 accuracy >> 0.0\niter >> 227 ,loss >> 3.5557003 accuracy >> 0.0\niter >> 228 ,loss >> 3.5859303 accuracy >> 0.125\niter >> 229 ,loss >> 3.627376 accuracy >> 0.0\niter >> 230 ,loss >> 3.0281675 accuracy >> 0.125\niter >> 231 ,loss >> 3.4440808 accuracy >> 0.125\niter >> 232 ,loss >> 3.126507 accuracy >> 0.0\niter >> 233 ,loss >> 3.1805685 accuracy >> 0.0\niter >> 234 ,loss >> 3.5235503 accuracy >> 0.0\niter >> 235 ,loss >> 3.6368856 accuracy >> 0.0\niter >> 236 ,loss >> 3.0989308 accuracy >> 0.0\niter >> 237 ,loss >> 3.548152 accuracy >> 0.0\niter >> 238 ,loss >> 3.590025 accuracy >> 0.0\niter >> 239 ,loss >> 3.5578282 accuracy >> 0.0\niter >> 240 ,loss >> 3.581645 accuracy >> 0.0\niter >> 241 ,loss >> 3.6770601 accuracy >> 0.0\niter >> 242 ,loss >> 3.5262802 accuracy >> 0.125\niter >> 243 ,loss >> 2.673119 accuracy >> 0.0\niter >> 244 ,loss >> 3.099709 accuracy >> 0.125\niter >> 245 ,loss >> 3.150637 accuracy >> 0.0\niter >> 246 ,loss >> 3.525527 accuracy >> 0.0\niter >> 247 ,loss >> 3.6192513 accuracy >> 0.125\niter >> 248 ,loss >> 3.5839226 accuracy >> 0.0\niter >> 249 ,loss >> 3.5714164 accuracy >> 0.0\niter >> 250 ,loss >> 3.1310523 accuracy >> 0.0\niter >> 251 ,loss >> 3.699815 accuracy >> 0.0\niter >> 252 ,loss >> 3.513349 accuracy >> 0.125\niter >> 253 ,loss >> 3.0987139 accuracy >> 0.0\niter >> 254 ,loss >> 3.5172815 accuracy >> 0.125\niter >> 255 ,loss >> 3.6425629 accuracy >> 0.0\niter >> 256 ,loss >> 3.6221695 accuracy >> 0.0\niter >> 257 ,loss >> 3.1462736 accuracy >> 0.125\niter >> 258 ,loss >> 3.4874575 accuracy >> 0.125\niter >> 259 ,loss >> 3.5518937 accuracy >> 0.125\niter >> 260 ,loss >> 3.5180535 accuracy >> 0.0\niter >> 261 ,loss >> 3.4831116 accuracy >> 0.0\niter >> 262 ,loss >> 3.5912838 accuracy >> 0.0\niter >> 263 ,loss >> 3.475858 accuracy >> 0.125\niter >> 264 ,loss >> 3.5519054 accuracy >> 0.125\niter >> 265 ,loss >> 3.5886555 accuracy >> 0.125\niter >> 266 ,loss >> 3.0656965 accuracy >> 0.125\niter >> 267 ,loss >> 3.1765704 accuracy >> 0.0\niter >> 268 ,loss >> 3.6214552 accuracy >> 0.0\niter >> 269 ,loss >> 3.080365 accuracy >> 0.0\niter >> 270 ,loss >> 3.589178 accuracy >> 0.0\niter >> 271 ,loss >> 3.5392795 accuracy >> 0.125\niter >> 272 ,loss >> 3.5664191 accuracy >> 0.0\niter >> 273 ,loss >> 3.6406896 accuracy >> 0.125\niter >> 274 ,loss >> 3.5702553 accuracy >> 0.0\niter >> 275 ,loss >> 3.5399933 accuracy >> 0.0\niter >> 276 ,loss >> 3.5080938 accuracy >> 0.0\niter >> 277 ,loss >> 3.5999446 accuracy >> 0.0\niter >> 278 ,loss >> 3.6165643 accuracy >> 0.0\niter >> 279 ,loss >> 3.2059155 accuracy >> 0.0\niter >> 280 ,loss >> 3.5299084 accuracy >> 0.375\niter >> 281 ,loss >> 3.452628 accuracy >> 0.0\niter >> 282 ,loss >> 3.474408 accuracy >> 0.0\niter >> 283 ,loss >> 3.4776483 accuracy >> 0.0\niter >> 284 ,loss >> 3.5916784 accuracy >> 0.0\niter >> 285 ,loss >> 3.505826 accuracy >> 0.0\niter >> 286 ,loss >> 3.5056849 accuracy >> 0.0\niter >> 287 ,loss >> 3.7176049 accuracy >> 0.0\niter >> 288 ,loss >> 3.5463026 accuracy >> 0.0\niter >> 289 ,loss >> 3.4573593 accuracy >> 0.25\niter >> 290 ,loss >> 3.4395604 accuracy >> 0.125\niter >> 291 ,loss >> 3.585175 accuracy >> 0.0\niter >> 292 ,loss >> 3.4972515 accuracy >> 0.0\niter >> 293 ,loss >> 3.1315885 accuracy >> 0.0\niter >> 294 ,loss >> 3.5580187 accuracy >> 0.0\niter >> 295 ,loss >> 3.188531 accuracy >> 0.0\niter >> 296 ,loss >> 3.6808813 accuracy >> 0.0\niter >> 297 ,loss >> 3.6581955 accuracy >> 0.0\niter >> 298 ,loss >> 3.448991 accuracy >> 0.0\niter >> 299 ,loss >> 3.5339117 accuracy >> 0.0\niter >> 300 ,loss >> 3.5202699 accuracy >> 0.0\niter >> 301 ,loss >> 3.475863 accuracy >> 0.0\niter >> 302 ,loss >> 3.621189 accuracy >> 0.0\niter >> 303 ,loss >> 3.0192285 accuracy >> 0.125\niter >> 304 ,loss >> 3.5971248 accuracy >> 0.0\niter >> 305 ,loss >> 3.580138 accuracy >> 0.0\niter >> 306 ,loss >> 3.5531719 accuracy >> 0.0\niter >> 307 ,loss >> 3.6042562 accuracy >> 0.0\niter >> 308 ,loss >> 3.514893 accuracy >> 0.0\niter >> 309 ,loss >> 3.1002116 accuracy >> 0.0\niter >> 310 ,loss >> 3.53014 accuracy >> 0.25\niter >> 311 ,loss >> 3.5641575 accuracy >> 0.0\niter >> 312 ,loss >> 3.7118518 accuracy >> 0.0\niter >> 313 ,loss >> 3.1138787 accuracy >> 0.0\niter >> 314 ,loss >> 3.5440369 accuracy >> 0.0\niter >> 315 ,loss >> 3.6339293 accuracy >> 0.0\niter >> 316 ,loss >> 3.5705357 accuracy >> 0.0\niter >> 317 ,loss >> 3.4926627 accuracy >> 0.125\niter >> 318 ,loss >> 3.5532875 accuracy >> 0.0\niter >> 319 ,loss >> 3.0413346 accuracy >> 0.125\niter >> 320 ,loss >> 3.6791792 accuracy >> 0.0\niter >> 321 ,loss >> 3.525386 accuracy >> 0.125\niter >> 322 ,loss >> 3.6076465 accuracy >> 0.0\niter >> 323 ,loss >> 3.524108 accuracy >> 0.0\niter >> 324 ,loss >> 3.6286004 accuracy >> 0.0\niter >> 325 ,loss >> 3.5876815 accuracy >> 0.125\niter >> 326 ,loss >> 3.7415895 accuracy >> 0.0\niter >> 327 ,loss >> 3.5330877 accuracy >> 0.25\niter >> 328 ,loss >> 3.4877086 accuracy >> 0.125\niter >> 329 ,loss >> 3.1625206 accuracy >> 0.0\niter >> 330 ,loss >> 3.5857363 accuracy >> 0.0\niter >> 331 ,loss >> 3.6148922 accuracy >> 0.0\niter >> 332 ,loss >> 3.5747244 accuracy >> 0.0\niter >> 333 ,loss >> 3.5972962 accuracy >> 0.0\niter >> 334 ,loss >> 3.1563885 accuracy >> 0.0\niter >> 335 ,loss >> 3.5671198 accuracy >> 0.0\niter >> 336 ,loss >> 3.5303307 accuracy >> 0.0\niter >> 337 ,loss >> 3.5303168 accuracy >> 0.125\niter >> 338 ,loss >> 3.5932631 accuracy >> 0.0\niter >> 339 ,loss >> 3.1565168 accuracy >> 0.0\niter >> 340 ,loss >> 3.0806565 accuracy >> 0.125\niter >> 341 ,loss >> 3.5362682 accuracy >> 0.0\niter >> 342 ,loss >> 3.4658043 accuracy >> 0.125\niter >> 343 ,loss >> 3.6109035 accuracy >> 0.0\niter >> 344 ,loss >> 3.5750384 accuracy >>0.0\niter >> 345 ,loss >> 3.5130322 accuracy >> 0.0\niter >> 346 ,loss >> 3.5169816 accuracy >> 0.0\niter >> 347 ,loss >> 3.089427 accuracy >> 0.0\niter >> 348 ,loss >> 3.102224 accuracy >> 0.0\niter >> 349 ,loss >> 3.158934 accuracy >> 0.0\niter >> 350 ,loss >> 3.6054592 accuracy >> 0.0\niter >> 351 ,loss >> 3.6669028 accuracy >> 0.0\niter >> 352 ,loss >> 3.4865708 accuracy >> 0.0\niter >> 353 ,loss >> 3.541543 accuracy >> 0.0\niter >> 354 ,loss >> 3.5875509 accuracy >> 0.0\niter >> 355 ,loss >> 3.4752119 accuracy >> 0.125\niter >> 356 ,loss >> 3.5661445 accuracy >> 0.0\niter >> 357 ,loss >> 3.1353416 accuracy >> 0.0\niter >> 358 ,loss >> 3.6072721 accuracy >> 0.0\niter >> 359 ,loss >> 3.5824523 accuracy >> 0.0\niter >> 360 ,loss >> 3.5424075 accuracy >> 0.0\niter >> 361 ,loss >> 3.0032563 accuracy >> 0.125\niter >> 362 ,loss >> 3.6082222 accuracy >> 0.0\niter >> 363 ,loss >> 3.5277252 accuracy >> 0.0\niter >> 364 ,loss >> 3.5569549 accuracy >> 0.0\niter >> 365 ,loss >> 3.4596376 accuracy >> 0.125\niter >> 366 ,loss >> 3.0796351 accuracy >> 0.0\niter >> 367 ,loss >> 3.102012 accuracy >> 0.0\niter >> 368 ,loss >> 2.999939 accuracy >> 0.0\niter >> 369 ,loss >> 3.4521632 accuracy >> 0.125\niter >> 370 ,loss >> 3.60324 accuracy >> 0.0\niter >> 371 ,loss >> 3.1114688 accuracy >> 0.0\niter >> 372 ,loss >> 3.4151769 accuracy >> 0.0\niter >> 373 ,loss >> 3.1657095 accuracy >> 0.0\niter >> 374 ,loss >> 3.498216 accuracy >> 0.125\niter >> 375 ,loss >> 3.5097795 accuracy >> 0.0\niter >> 376 ,loss >> 3.5746727 accuracy >> 0.125\niter >> 377 ,loss >> 3.6117535 accuracy >> 0.0\niter >> 378 ,loss >> 3.1632006 accuracy >> 0.0\niter >> 379 ,loss >> 3.3920312 accuracy >> 0.125\niter >> 380 ,loss >> 3.1855881 accuracy >> 0.0\niter >> 381 ,loss >> 3.0366063 accuracy >> 0.0\niter >> 382 ,loss >> 3.5915956 accuracy >> 0.0\niter >> 383 ,loss >> 3.4499009 accuracy >> 0.25\niter >> 384 ,loss >> 3.5914989 accuracy >> 0.0\niter >> 385 ,loss >> 3.5248895 accuracy >> 0.0\niter >> 386 ,loss >> 3.5757272 accuracy >> 0.0\niter >> 387 ,loss >> 2.973466 accuracy >> 0.0\niter >> 388 ,loss >> 3.1198044 accuracy >> 0.125\niter >> 389 ,loss >> 3.3451958 accuracy >> 0.125\niter >> 390 ,loss >> 3.557949 accuracy >> 0.0\niter >> 391 ,loss >> 3.0295405 accuracy >> 0.0\niter >> 392 ,loss >> 3.653181 accuracy >> 0.125\niter >> 393 ,loss >> 3.5652657 accuracy >> 0.0\niter >> 394 ,loss >> 3.484488 accuracy >> 0.0\niter >> 395 ,loss >> 3.5515318 accuracy >> 0.125\niter >> 396 ,loss >> 3.4714425 accuracy >> 0.0\niter >> 397 ,loss >> 3.632506 accuracy >> 0.0\niter >> 398 ,loss >> 3.5613117 accuracy >> 0.0\niter >> 399 ,loss >> 3.6526144 accuracy >> 0.0\niter >> 400 ,loss >> 3.3813014 accuracy >> 0.125\niter >> 401 ,loss >> 3.4953346 accuracy >> 0.0\niter >> 402 ,loss >> 3.6863241 accuracy >> 0.0\niter >> 403 ,loss >> 3.7366376 accuracy >> 0.0\niter >> 404 ,loss >> 3.04854 accuracy >> 0.0\niter >> 405 ,loss >> 3.4039316 accuracy >> 0.375\niter >> 406 ,loss >> 3.5131984 accuracy >> 0.0\niter >> 407 ,loss >> 3.1416912 accuracy >> 0.125\niter >> 408 ,loss >> 3.6355898 accuracy >> 0.0\niter >> 409 ,loss >> 3.5532107 accuracy >> 0.125\niter >> 410 ,loss >> 3.5082603 accuracy >>0.0\niter >> 411 ,loss >> 3.5137515 accuracy >> 0.0\niter >> 412 ,loss >> 3.5534096 accuracy >> 0.0\niter >> 413 ,loss >> 2.6480193 accuracy >> 0.125\niter >> 414 ,loss >> 3.5042138 accuracy >> 0.125\niter >> 415 ,loss >> 3.0799074 accuracy >> 0.0\niter >> 416 ,loss >> 3.5201573 accuracy >> 0.125\niter >> 417 ,loss >> 3.090002 accuracy >> 0.0\niter >> 418 ,loss >> 3.5444665 accuracy >> 0.125\niter >> 419 ,loss >> 3.064361 accuracy >> 0.0\niter >> 420 ,loss >> 3.5208654 accuracy >> 0.125\niter >> 421 ,loss >> 3.4509056 accuracy >> 0.125\niter >> 422 ,loss >> 3.4792128 accuracy >> 0.125\niter >> 423 ,loss >> 3.6139953 accuracy >> 0.0\niter >> 424 ,loss >> 3.5738716 accuracy >> 0.125\niter >> 425 ,loss >> 3.5294628 accuracy >> 0.125\niter >> 426 ,loss >> 3.5223129 accuracy >> 0.125\niter >> 427 ,loss >> 3.5855012 accuracy >> 0.125\niter >> 428 ,loss >> 3.1113462 accuracy >> 0.0\niter >> 429 ,loss >> 3.5421789 accuracy >> 0.0\niter >> 430 ,loss >> 3.501263 accuracy >> 0.0\niter >> 431 ,loss >> 3.0137515 accuracy >> 0.125\niter >> 432 ,loss >> 3.5576444 accuracy >> 0.0\niter >> 433 ,loss >> 3.1666446 accuracy >> 0.0\niter >> 434 ,loss >> 3.0291946 accuracy >> 0.125\niter >> 435 ,loss >> 3.5897503 accuracy >> 0.0\niter >> 436 ,loss >> 3.5732756 accuracy >> 0.0\niter >> 437 ,loss >> 3.5191934 accuracy >> 0.0\niter >> 438 ,loss >> 3.3927774 accuracy >> 0.0\niter >> 439 ,loss >> 3.5589004 accuracy >> 0.0\niter >> 440 ,loss >> 3.4831758 accuracy >> 0.0\niter >> 441 ,loss >> 3.5079806 accuracy >> 0.0\niter >> 442 ,loss >> 3.4330153 accuracy >> 0.25\niter >> 443 ,loss >> 3.0466468 accuracy >> 0.125\niter >> 444 ,loss >> 3.1302786 accuracy >> 0.0\niter >> 445 ,loss >> 3.3968303 accuracy >> 0.0\niter >> 446 ,loss >> 3.4857874 accuracy >> 0.125\niter >> 447 ,loss >> 3.609777 accuracy >> 0.125\niter >> 448 ,loss >> 3.5137753 accuracy >> 0.125\niter >> 449 ,loss >> 3.5915842 accuracy >> 0.0\niter >> 450 ,loss >> 3.4353971 accuracy >> 0.125\niter >> 451 ,loss >> 3.3528562 accuracy >> 0.0\niter >> 452 ,loss >> 3.407459 accuracy >> 0.0\niter >> 453 ,loss >> 3.1330833 accuracy >> 0.0\niter >> 454 ,loss >> 3.6400356 accuracy >> 0.0\niter >> 455 ,loss >> 3.4830523 accuracy >> 0.0\niter >> 456 ,loss >> 3.4214041 accuracy >> 0.125\niter >> 457 ,loss >> 3.147604 accuracy >> 0.0\niter >> 458 ,loss >> 3.3834357 accuracy >> 0.125\niter >> 459 ,loss >> 3.6416101 accuracy >> 0.0\niter >> 460 ,loss >> 3.387353 accuracy >> 0.0\niter >> 461 ,loss >> 3.5686858 accuracy >> 0.0\niter >> 462 ,loss >> 3.4096937 accuracy >> 0.125\niter >> 463 ,loss >> 3.5074456 accuracy >> 0.0\niter >> 464 ,loss >> 3.6736379 accuracy >> 0.0\niter >> 465 ,loss >> 3.543973 accuracy >> 0.0\niter >> 466 ,loss >> 3.3902845 accuracy >> 0.0\niter >> 467 ,loss >> 3.3918917 accuracy >> 0.0\niter >> 468 ,loss >> 2.682116 accuracy >> 0.0\niter >> 469 ,loss >> 3.48358 accuracy >> 0.125\niter >> 470 ,loss >> 3.583593 accuracy >> 0.0\niter >> 471 ,loss >> 3.0025082 accuracy >> 0.0\niter >> 472 ,loss >> 3.4438217 accuracy >> 0.0\niter >> 473 ,loss >> 3.0837078 accuracy >> 0.0\niter >> 474 ,loss >> 3.542447 accuracy >> 0.0\niter >> 475 ,loss >> 3.585753 accuracy >> 0.0\niter >> 476 ,loss >> 3.2981853 accuracy >> 0.125\niter >> 477 ,loss >> 3.401742 accuracy >> 0.0\niter >> 478 ,loss >> 3.377112 accuracy >> 0.125\niter >> 479 ,loss >> 3.4905183 accuracy >> 0.125\niter >> 480 ,loss >> 3.2938828 accuracy >> 0.125\niter >> 481 ,loss >> 3.4054508 accuracy >> 0.0\niter >> 482 ,loss >> 2.9974413 accuracy >> 0.0\niter >> 483 ,loss >> 3.0646386 accuracy >> 0.0\niter >> 484 ,loss >> 3.479805 accuracy >> 0.0\niter >> 485 ,loss >> 2.9558406 accuracy >> 0.0\niter >> 486 ,loss >> 3.1578255 accuracy >> 0.0\niter >> 487 ,loss >> 3.3041296 accuracy >> 0.25\niter >> 488 ,loss >> 3.4132094 accuracy >> 0.0\niter >> 489 ,loss >> 2.989557 accuracy >> 0.125\niter >> 490 ,loss >> 3.8384027 accuracy >> 0.125\niter >> 491 ,loss >> 3.7776608 accuracy >> 0.0\niter >> 492 ,loss >> 3.1753201 accuracy >> 0.125\niter >> 493 ,loss >> 3.518893 accuracy >> 0.0\niter >> 494 ,loss >> 3.1129394 accuracy >> 0.0\niter >> 495 ,loss >> 3.710081 accuracy >> 0.0\niter >> 496 ,loss >> 3.1538563 accuracy >> 0.375\niter >> 497 ,loss >> 3.6029668 accuracy >> 0.0\niter >> 498 ,loss >> 3.6010003 accuracy >> 0.0\niter >> 499 ,loss >> 3.549873 accuracy >> 0.0\niter >> 500 ,loss >> 3.2751238 accuracy >> 0.25\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "# データを読み込むフォルダを指定する\n",
    "DirPath = '../../../DataSet/AngleDetection/training/*'\n",
    "#print(DirPath)\n",
    "\n",
    "num_classes = 36\n",
    "img_width, img_height = 64, 64\n",
    "CLS = np.arange(0, 175, 5).astype('str')\n",
    "\n",
    "#train(DirPath, img_size=(64, 64), cls_label=CLS)\n",
    "train(DirPath, img_size=(img_width, img_height), cls_label=CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}