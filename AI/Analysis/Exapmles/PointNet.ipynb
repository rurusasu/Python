{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd00eecd040b31a140d4c1a4abfb9821f369b6a9e45dafd8a2bb8398d6490c0b8f2",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PointNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## モデル構築用パッケージをインストール"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "source": [
    "## Architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(NonLinear, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.input_channels, self.output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(self.output_channels))\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        return self.main(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool(nn.Module):\n",
    "    def __init__(self, num_channels, num_points):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_points = num_points\n",
    "        self.main = nn.MaxPool1d(self.num_points)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        out = input_data.view(-1, self.num_channels, self.num_points)\n",
    "        out = self.main(out)\n",
    "        out = out.view(-1, self.num_channels)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputTNet(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(InputTNet, self).__init__()\n",
    "        self.num_points = num_points\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            NonLinear(3, 64),\n",
    "            NonLinear(64, 128),\n",
    "            NonLinear(128, 1024),\n",
    "            MaxPool(1024, self.num_points),\n",
    "            NonLinear(1024, 512),\n",
    "            NonLinear(512, 256),\n",
    "            nn.Linear(256, 9)\n",
    "        )\n",
    "\n",
    "    # shape of input_data is (batchsize x num_points, channel)\n",
    "    def forward(self, input_data):\n",
    "        matrix = self.main(input_data).view(-1, 3, 3)\n",
    "        out = torch.matmul(input_data.view(-1, self.num_points, 3), matrix)\n",
    "        out = out.view(-1, 3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTNet(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(FeatureTNet, self).__init__()\n",
    "        self.num_points = num_points\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            NonLinear(64, 64),\n",
    "            NonLinear(64, 128),\n",
    "            NonLinear(128, 1024),\n",
    "            MaxPool(1024, self.num_points),\n",
    "            NonLinear(1024, 512),\n",
    "            NonLinear(512, 256),\n",
    "            nn.Linear(256, 4096)\n",
    "        )\n",
    "\n",
    "    # shape of input_data is (batchsize x num_points, channel)\n",
    "    def forward(self, input_data):\n",
    "        matrix = self.main(input_data).view(-1, 64, 64)\n",
    "        out = torch.matmul(input_data.view(-1, self.num_points, 64), matrix)\n",
    "        out = out.view(-1, 64)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, num_points, num_labels):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            InputTNet(self.num_points),\n",
    "            NonLinear(3, 64),\n",
    "            NonLinear(64, 64),\n",
    "            FeatureTNet(self.num_points),\n",
    "            NonLinear(64, 64),\n",
    "            NonLinear(64, 128),\n",
    "            NonLinear(128, 1024),\n",
    "            MaxPool(1024, self.num_points),\n",
    "            NonLinear(1024, 512),\n",
    "            nn.Dropout(p = 0.3),\n",
    "            NonLinear(512, 256),\n",
    "            nn.Dropout(p = 0.3),\n",
    "            NonLinear(256, self.num_labels),\n",
    "            )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        return self.main(input_data)"
   ]
  },
  {
   "source": [
    "## Sample Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampler(batch_size, num_points):\n",
    "    half_batch_size = int(batch_size/2)\n",
    "    normal_sampled = torch.randn(half_batch_size, num_points, 3)\n",
    "    uniform_sampled = torch.rand(half_batch_size, num_points, 3)\n",
    "    normal_labels = torch.ones(half_batch_size)\n",
    "    uniform_labels = torch.zeros(half_batch_size)\n",
    "\n",
    "    input_data = torch.cat((normal_sampled, uniform_sampled), dim=0)\n",
    "    labels = torch.cat((normal_labels, uniform_labels), dim=0)\n",
    "\n",
    "    data_shuffle = torch.randperm(batch_size)\n",
    "  \n",
    "    return input_data[data_shuffle].view(-1, 3), labels[data_shuffle].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_points = 64\n",
    "num_labels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0.0225, -0.0600, -0.0314,  0.0354, -0.0047, -0.0578, -0.0469,  0.0073,\n         0.0413])\n"
     ]
    }
   ],
   "source": [
    "pointnet = PointNet(num_points, num_labels)\n",
    "new_param = pointnet.state_dict()\n",
    "print(new_param['main.0.main.6.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2e-03, -1.9187e-02,\n         3.0759e-02,  2.2969e-02,  3.0003e-02,  2.9062e-02, -1.1159e-02,\n        -1.7297e-02, -2.3087e-02, -1.2868e-02,  2.7561e-02, -1.7277e-02,\n        -7.9646e-03, -1.5188e-02,  6.1635e-03, -2.4922e-02,  7.1074e-03,\n        -2.5371e-02,  6.7718e-03, -1.3580e-02, -1.4029e-02,  2.7437e-02,\n         1.7473e-03,  2.6656e-02, -1.6679e-03,  1.1729e-02, -2.4906e-02,\n         1.6040e-02, -9.7106e-03,  2.2595e-02,  1.2265e-02,  1.0617e-02,\n         1.6129e-02, -1.3345e-02,  3.6715e-04,  1.2542e-03,  2.2798e-02,\n        -1.7539e-02,  2.3377e-02, -1.1164e-02, -2.0425e-02,  1.2335e-02,\n         1.2354e-02,  3.0750e-03,  2.3295e-02,  2.5754e-02, -3.0740e-02,\n        -1.6611e-02,  3.0707e-02,  5.3548e-04, -3.0337e-02, -1.9122e-02,\n        -6.4766e-03,  6.2205e-04, -2.9238e-02, -2.9834e-02,  9.9700e-03,\n         1.2641e-03, -1.9878e-02, -2.9550e-03, -8.6126e-03,  1.0351e-02,\n         2.8127e-02, -7.6938e-03, -2.9751e-02,  1.9012e-02, -1.9640e-02,\n        -1.3597e-02,  1.4651e-02, -2.4036e-02, -2.8003e-02,  1.7399e-02,\n         2.8895e-02,  2.8595e-02,  2.3803e-02,  3.0682e-02,  1.9598e-02,\n        -2.5771e-02, -9.0580e-03, -2.4037e-02,  2.7289e-02, -2.3055e-02,\n        -2.1866e-03, -1.5627e-02, -9.9045e-03,  2.8870e-02,  2.5972e-02,\n         2.6672e-02,  1.6558e-02,  1.5531e-02,  1.4509e-02, -2.9974e-02,\n         2.9139e-02, -9.5073e-03, -1.0946e-02,  2.1194e-03, -7.8422e-03,\n         1.8914e-02, -2.8510e-02,  1.4389e-02, -1.0461e-04, -1.9822e-02,\n         1.4868e-02, -1.2428e-02, -2.4666e-02, -1.0347e-02,  6.3957e-04,\n        -1.9715e-02,  2.9839e-02,  1.2947e-02, -3.0637e-02, -2.5953e-02,\n        -1.3101e-03, -2.0560e-02, -9.3920e-04, -6.5503e-03,  1.1109e-02,\n        -2.2427e-02, -1.1397e-02, -1.4818e-03, -3.2525e-03,  5.5899e-03,\n        -2.2282e-02,  3.7739e-03, -1.8586e-02, -8.5242e-03,  9.0937e-03,\n        -1.2606e-02, -3.0009e-02, -2.4578e-02,  2.3982e-02, -2.6644e-02,\n        -1.0495e-02,  1.4163e-02, -2.7449e-02,  2.2564e-03,  1.2970e-02,\n         1.4666e-02, -6.0345e-03,  1.7203e-02, -3.5559e-03, -2.7542e-02,\n         6.2531e-03,  8.5422e-03,  1.7109e-03, -2.1874e-02, -1.9431e-02,\n        -2.6952e-02, -2.8600e-02,  2.4627e-02,  8.8554e-03,  2.9379e-02,\n        -9.3738e-03,  8.9348e-03,  1.1679e-02, -5.8402e-03,  1.6965e-02,\n         1.4958e-02, -3.8521e-03,  2.3301e-02,  7.4413e-03,  8.9621e-03,\n        -1.4871e-03, -1.0170e-02, -2.3992e-02,  4.3017e-04, -2.1756e-03,\n        -6.0760e-03,  1.4407e-02, -2.8552e-02,  1.3324e-02, -2.5394e-02,\n         2.5029e-02, -1.5502e-02, -1.5218e-02, -1.6563e-02,  2.2060e-02,\n        -2.5190e-03, -5.1898e-03,  6.0962e-03,  2.6047e-02, -8.7281e-03,\n         9.5994e-03,  2.2873e-04,  2.8213e-02,  1.3983e-02, -1.5207e-02,\n        -2.0920e-02,  2.1162e-02,  1.2056e-02,  1.0307e-02,  7.0563e-04,\n        -2.5435e-03, -1.9127e-02,  1.9553e-02,  1.1472e-02, -4.3763e-03,\n        -2.2737e-02, -1.5663e-02,  9.2902e-03,  2.5390e-02,  1.3386e-02,\n        -4.8095e-03, -1.3544e-02, -1.8333e-02, -1.2650e-02,  1.3476e-02,\n        -2.5973e-02,  2.6307e-02, -1.9668e-02,  5.7075e-03, -2.3020e-03,\n        -9.8490e-03, -3.5157e-03,  1.4888e-02, -1.8234e-02, -2.8263e-02,\n         7.4476e-03, -2.4283e-02,  9.5972e-03, -8.6236e-03,  1.3293e-02,\n         7.8731e-03, -1.8180e-02,  2.4557e-02,  4.3165e-03,  2.5856e-02,\n         1.1962e-02,  1.2519e-03,  1.0828e-02, -2.9866e-02,  2.2655e-02,\n         2.8952e-02, -1.7599e-02,  1.9909e-02, -2.1387e-02, -8.7321e-03,\n        -2.5880e-02,  1.0737e-02, -9.8816e-03, -9.0118e-03,  3.0863e-03,\n        -2.6583e-03, -1.0473e-02, -2.3970e-02,  2.6839e-02,  2.8089e-02,\n        -3.7874e-03, -1.0959e-02, -6.2905e-03,  1.9668e-02,  4.6070e-03,\n        -4.0119e-03,  2.8398e-02, -1.9722e-02,  3.0292e-03,  7.1839e-03,\n        -2.5490e-02,  2.7926e-02,  1.7022e-02, -6.3353e-03, -3.4274e-03,\n        -2.9477e-02, -6.3694e-03,  2.0847e-03, -4.2972e-03, -2.6747e-02,\n        -2.4159e-02,  1.5875e-02,  2.4586e-02, -2.9398e-03,  2.2913e-02,\n        -9.4839e-03,  1.2780e-02,  1.6024e-02,  1.1675e-02,  1.5368e-02,\n         2.0669e-03, -2.0882e-02,  2.5015e-02, -5.3676e-03,  1.1171e-02,\n         2.9939e-02, -2.0574e-03,  1.1330e-02,  2.4746e-02,  2.6577e-02,\n         1.3457e-02, -1.3887e-03, -2.8937e-02, -8.8445e-03, -2.6181e-02,\n        -2.6793e-04, -2.7336e-02,  1.4687e-02,  1.4338e-02, -2.8248e-02,\n        -2.2885e-02,  1.1357e-02,  2.3446e-02, -8.1304e-05,  2.5960e-02,\n        -1.6372e-02, -1.1273e-03,  1.1713e-02,  4.1376e-03, -6.5093e-03,\n        -1.5848e-03,  1.3095e-03,  2.0173e-02,  2.1500e-02,  3.7225e-03,\n        -2.1413e-02,  2.4234e-02, -9.8268e-03,  2.5182e-02, -5.6929e-03,\n         2.2140e-02,  5.9225e-03, -1.5846e-02, -2.7580e-02, -3.8510e-03,\n         4.7900e-03,  3.0816e-02, -1.6190e-03,  2.5643e-02, -2.9322e-02,\n         1.7105e-02,  2.8568e-02, -2.8078e-02,  8.1807e-04,  2.1524e-02,\n        -1.3520e-02, -1.4512e-04,  9.6901e-03,  2.2947e-02, -1.5053e-02,\n        -2.9385e-02,  3.0535e-02, -1.7146e-02,  4.0411e-03, -2.4137e-03,\n         9.7022e-03,  6.4719e-03, -2.0965e-02,  1.8384e-02, -2.8585e-02,\n        -2.7874e-02,  3.1230e-02,  1.2955e-02,  2.0394e-02, -2.7324e-02,\n         1.3535e-02, -2.8019e-02, -5.6320e-03, -1.4909e-02,  5.6412e-03,\n         3.1214e-02,  3.0552e-02,  6.4317e-03, -1.6395e-02, -2.4244e-02,\n         1.9223e-02,  7.9530e-03, -1.6187e-02,  1.4930e-02,  2.7813e-02,\n        -8.3798e-04,  7.7784e-03,  2.3189e-03,  2.2256e-02, -2.9915e-02,\n         4.5019e-03, -7.2525e-03,  3.8236e-03,  2.9224e-04, -2.5817e-02,\n        -2.2281e-02, -2.8018e-02, -1.5139e-03,  2.0400e-02,  1.0246e-02,\n        -1.3335e-03, -8.8131e-03,  1.2491e-03,  2.1897e-02, -3.8578e-03,\n        -1.7569e-02, -2.7351e-02, -2.1545e-03,  6.0015e-03, -2.4105e-02,\n        -2.0174e-02, -2.4715e-02, -4.4313e-03, -2.3942e-02,  2.9709e-02,\n        -6.0194e-03, -3.0419e-02, -9.4526e-03, -1.0107e-03,  1.6483e-02,\n        -3.1055e-02,  1.5943e-02,  1.5933e-03,  7.4111e-03, -1.5538e-02,\n        -4.1563e-03, -3.0099e-02,  8.8074e-03, -3.0668e-02,  1.3371e-02,\n        -2.0366e-02, -6.1746e-03, -2.5996e-02,  1.6622e-02,  1.8846e-02,\n         1.7553e-03,  1.7415e-02, -2.0654e-02, -2.8078e-02, -9.6411e-03,\n        -2.8539e-02, -1.9515e-03, -1.9364e-02,  1.0942e-03, -3.0425e-02,\n         2.4444e-02, -2.3031e-02, -2.1889e-02,  2.8946e-04,  1.6749e-02,\n        -4.4091e-03, -2.9424e-02, -2.9648e-02,  1.2929e-02,  2.7747e-02,\n         2.0946e-02,  2.1027e-02, -3.1110e-02, -2.6665e-02, -4.3236e-03,\n        -2.8432e-02,  8.3181e-03, -1.2777e-02,  2.5910e-02,  8.0651e-03,\n        -5.1534e-03,  2.8746e-02,  2.5633e-02, -2.8166e-02, -2.1934e-02,\n        -4.4556e-03, -1.7512e-02, -1.3982e-02,  3.3656e-03,  1.6665e-02,\n         2.7908e-02, -2.3197e-02,  1.0583e-02,  6.0890e-03,  2.7105e-03,\n         2.4649e-02,  1.6780e-02,  7.6882e-03, -7.6608e-03, -2.8148e-02,\n         2.5235e-02, -2.7859e-02, -1.4566e-02,  2.0933e-02, -1.4939e-02,\n        -2.3555e-04,  6.4052e-04, -1.2934e-02,  2.3250e-02, -1.8377e-03,\n        -1.0556e-02, -1.7767e-02, -1.2546e-02,  2.9507e-02, -8.0476e-03,\n        -5.7907e-03, -6.6406e-03,  2.4332e-02, -1.9778e-02,  7.0887e-03,\n         4.0050e-03,  7.5538e-03], requires_grad=True)\nParameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\nParameter containing:\ntensor([[-0.0353, -0.0407,  0.0142,  ..., -0.0441, -0.0266, -0.0172],\n        [ 0.0210, -0.0236,  0.0430,  ..., -0.0061, -0.0351, -0.0149],\n        [-0.0104, -0.0328, -0.0141,  ...,  0.0356,  0.0164,  0.0209],\n        ...,\n        [-0.0270, -0.0233,  0.0185,  ...,  0.0164,  0.0205, -0.0147],\n        [ 0.0292, -0.0238,  0.0164,  ..., -0.0220,  0.0234,  0.0167],\n        [ 0.0025, -0.0006,  0.0382,  ...,  0.0227, -0.0013, -0.0017]],\n       requires_grad=True)\nParameter containing:\ntensor([-0.0409, -0.0339, -0.0176, -0.0094, -0.0330, -0.0313,  0.0387,  0.0366,\n        -0.0331, -0.0233, -0.0241,  0.0260,  0.0409,  0.0214, -0.0185,  0.0257,\n        -0.0338, -0.0294,  0.0013,  0.0294,  0.0376, -0.0193, -0.0101, -0.0115,\n        -0.0128, -0.0038, -0.0172, -0.0339, -0.0301,  0.0359, -0.0277,  0.0248,\n         0.0437,  0.0234,  0.0335,  0.0337,  0.0337, -0.0414, -0.0085, -0.0052,\n        -0.0392, -0.0235, -0.0310,  0.0215,  0.0225,  0.0217,  0.0442, -0.0167,\n        -0.0144, -0.0150, -0.0032, -0.0061, -0.0434,  0.0293, -0.0350, -0.0169,\n         0.0196, -0.0433,  0.0324, -0.0264,  0.0383,  0.0203,  0.0272, -0.0397,\n        -0.0003,  0.0359,  0.0267, -0.0083, -0.0347, -0.0237, -0.0083,  0.0187,\n        -0.0186,  0.0193, -0.0289, -0.0355, -0.0151, -0.0046, -0.0397, -0.0359,\n         0.0273,  0.0363, -0.0325, -0.0135,  0.0230, -0.0386, -0.0231, -0.0211,\n         0.0058, -0.0140, -0.0045,  0.0130, -0.0130, -0.0094, -0.0106, -0.0154,\n        -0.0436, -0.0370,  0.0209,  0.0023, -0.0216, -0.0161, -0.0004,  0.0258,\n        -0.0053, -0.0163, -0.0393, -0.0256, -0.0033, -0.0034, -0.0150, -0.0137,\n         0.0033,  0.0150,  0.0402,  0.0261, -0.0359,  0.0178,  0.0268, -0.0009,\n         0.0035, -0.0056, -0.0411,  0.0154,  0.0004, -0.0231, -0.0197, -0.0139,\n        -0.0132,  0.0388,  0.0440, -0.0177,  0.0023,  0.0366,  0.0392, -0.0140,\n        -0.0348, -0.0039,  0.0209, -0.0138, -0.0339, -0.0244,  0.0344,  0.0170,\n         0.0402,  0.0112, -0.0055,  0.0144,  0.0215, -0.0080,  0.0246,  0.0076,\n        -0.0423,  0.0406,  0.0046, -0.0239,  0.0179, -0.0025,  0.0104,  0.0391,\n        -0.0293, -0.0042, -0.0128,  0.0340,  0.0005, -0.0408,  0.0262, -0.0390,\n         0.0128,  0.0368,  0.0334, -0.0320, -0.0252, -0.0302,  0.0127,  0.0185,\n        -0.0132, -0.0177, -0.0394, -0.0396,  0.0137,  0.0328,  0.0170,  0.0066,\n        -0.0127,  0.0230,  0.0314,  0.0330, -0.0365,  0.0173,  0.0299, -0.0208,\n        -0.0004, -0.0051,  0.0140, -0.0186, -0.0429,  0.0203,  0.0292, -0.0079,\n         0.0166, -0.0387,  0.0410, -0.0372, -0.0427, -0.0145, -0.0013,  0.0330,\n        -0.0186,  0.0330, -0.0144, -0.0275,  0.0121, -0.0246, -0.0108,  0.0012,\n         0.0335, -0.0315, -0.0023,  0.0244,  0.0058, -0.0397, -0.0216,  0.0356,\n        -0.0178,  0.0197,  0.0414, -0.0006, -0.0160, -0.0206, -0.0031, -0.0060,\n         0.0288, -0.0185,  0.0285,  0.0176, -0.0078,  0.0158,  0.0273,  0.0400,\n         0.0309, -0.0054,  0.0231,  0.0106, -0.0312,  0.0342, -0.0249,  0.0296,\n         0.0237, -0.0432, -0.0172,  0.0205, -0.0339,  0.0240, -0.0292,  0.0357],\n       requires_grad=True)\nParameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.], requires_grad=True)\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       requires_grad=True)\nParameter containing:\ntensor([[-0.0325, -0.0429,  0.0576,  0.0623, -0.0578,  0.0062, -0.0470, -0.0355,\n         -0.0529,  0.0500,  0.0449, -0.0015,  0.0158,  0.0602,  0.0622, -0.0246,\n          0.0384, -0.0316, -0.0091,  0.0363,  0.0039, -0.0229,  0.0498, -0.0585,\n          0.0512, -0.0189, -0.0082,  0.0116, -0.0149,  0.0507,  0.0352, -0.0492,\n         -0.0159, -0.0200,  0.0213, -0.0294, -0.0436,  0.0435, -0.0311, -0.0434,\n          0.0470, -0.0590,  0.0130,  0.0028, -0.0331,  0.0158, -0.0202,  0.0069,\n         -0.0402,  0.0076,  0.0524, -0.0586,  0.0262,  0.0332, -0.0321, -0.0198,\n         -0.0139, -0.0584, -0.0624, -0.0474,  0.0088, -0.0487,  0.0113, -0.0114,\n          0.0538,  0.0272,  0.0547,  0.0473,  0.0508,  0.0126,  0.0019,  0.0356,\n          0.0009, -0.0064,  0.0112,  0.0574,  0.0094, -0.0273, -0.0484, -0.0183,\n          0.0025, -0.0622, -0.0095, -0.0573,  0.0272, -0.0481,  0.0331,  0.0374,\n         -0.0577, -0.0090, -0.0341,  0.0409,  0.0592, -0.0190, -0.0472,  0.0154,\n          0.0317,  0.0067,  0.0027,  0.0276,  0.0427,  0.0408,  0.0104, -0.0349,\n         -0.0220,  0.0582, -0.0088, -0.0149,  0.0359,  0.0012, -0.0429,  0.0396,\n         -0.0462, -0.0141,  0.0047,  0.0545,  0.0186, -0.0432,  0.0512,  0.0095,\n          0.0467,  0.0022,  0.0054, -0.0266, -0.0198, -0.0253, -0.0188,  0.0174,\n         -0.0153, -0.0619, -0.0399, -0.0352, -0.0363, -0.0179, -0.0540,  0.0564,\n         -0.0601, -0.0414, -0.0285, -0.0509,  0.0621,  0.0427,  0.0532, -0.0012,\n         -0.0214, -0.0455, -0.0157, -0.0184,  0.0466,  0.0083,  0.0206,  0.0494,\n         -0.0403,  0.0464, -0.0231,  0.0381,  0.0421, -0.0007,  0.0345,  0.0284,\n         -0.0006,  0.0497,  0.0446,  0.0059, -0.0624, -0.0552,  0.0481, -0.0115,\n         -0.0110,  0.0085,  0.0149,  0.0230,  0.0101, -0.0252,  0.0226,  0.0565,\n          0.0047,  0.0297,  0.0598,  0.0090, -0.0462,  0.0543,  0.0584,  0.0296,\n         -0.0217,  0.0325, -0.0214,  0.0450, -0.0468,  0.0601, -0.0404,  0.0203,\n         -0.0311, -0.0440,  0.0041,  0.0199,  0.0337, -0.0488,  0.0137,  0.0272,\n          0.0264,  0.0439,  0.0583,  0.0148, -0.0107, -0.0194, -0.0186, -0.0116,\n          0.0077,  0.0589, -0.0348, -0.0582, -0.0184, -0.0517,  0.0277,  0.0447,\n         -0.0221,  0.0207,  0.0201,  0.0460,  0.0272,  0.0521,  0.0227, -0.0532,\n         -0.0454, -0.0034, -0.0082, -0.0216, -0.0361, -0.0514,  0.0048, -0.0267,\n          0.0451, -0.0392,  0.0283,  0.0317, -0.0595,  0.0390, -0.0405,  0.0073,\n          0.0218,  0.0544,  0.0242,  0.0488, -0.0375,  0.0065, -0.0575,  0.0066,\n         -0.0068, -0.0026,  0.0317, -0.0129,  0.0418,  0.0570, -0.0209,  0.0509]],\n       requires_grad=True)\nParameter containing:\ntensor([0.0623], requires_grad=True)\nParameter containing:\ntensor([1.], requires_grad=True)\nParameter containing:\ntensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in pointnet.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_param['main.0.main.6.bias'] = torch.eye(3, 3).view(-1)\n",
    "new_param['main.3.main.6.bias'] = torch.eye(64, 64).view(-1)\n",
    "pointnet.load_state_dict(new_param)\n",
    "\n",
    "criterion =  nn.BCELoss()\n",
    "optimizer = optim.Adam(pointnet.parameters(), lr=0.001)\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration : 0   Loss : 0.6997767686843872\n",
      "Iteration : 0   Accuracy : 0.53125\n",
      "Iteration : 10   Loss : 0.40883487462997437\n",
      "Iteration : 10   Accuracy : 0.96875\n",
      "Iteration : 20   Loss : 0.334423303604126\n",
      "Iteration : 20   Accuracy : 1.0\n",
      "Iteration : 30   Loss : 0.3193380832672119\n",
      "Iteration : 30   Accuracy : 1.0\n",
      "Iteration : 40   Loss : 0.31470927596092224\n",
      "Iteration : 40   Accuracy : 1.0\n",
      "Iteration : 50   Loss : 0.3165438771247864\n",
      "Iteration : 50   Accuracy : 1.0\n",
      "Iteration : 60   Loss : 0.30594080686569214\n",
      "Iteration : 60   Accuracy : 1.0\n",
      "Iteration : 70   Loss : 0.29978641867637634\n",
      "Iteration : 70   Accuracy : 1.0\n",
      "Iteration : 80   Loss : 0.2950426936149597\n",
      "Iteration : 80   Accuracy : 1.0\n",
      "Iteration : 90   Loss : 0.29417240619659424\n",
      "Iteration : 90   Accuracy : 1.0\n",
      "Iteration : 100   Loss : 0.29515373706817627\n",
      "Iteration : 100   Accuracy : 1.0\n",
      "Iteration : 110   Loss : 0.28817838430404663\n",
      "Iteration : 110   Accuracy : 1.0\n",
      "Iteration : 120   Loss : 0.2861238718032837\n",
      "Iteration : 120   Accuracy : 1.0\n",
      "Iteration : 130   Loss : 0.284702330827713\n",
      "Iteration : 130   Accuracy : 1.0\n",
      "Iteration : 140   Loss : 0.2800908386707306\n",
      "Iteration : 140   Accuracy : 1.0\n",
      "Iteration : 150   Loss : 0.27900299429893494\n",
      "Iteration : 150   Accuracy : 1.0\n",
      "Iteration : 160   Loss : 0.2759069800376892\n",
      "Iteration : 160   Accuracy : 1.0\n",
      "Iteration : 170   Loss : 0.27439209818840027\n",
      "Iteration : 170   Accuracy : 1.0\n",
      "Iteration : 180   Loss : 0.27126291394233704\n",
      "Iteration : 180   Accuracy : 1.0\n",
      "Iteration : 190   Loss : 0.26842084527015686\n",
      "Iteration : 190   Accuracy : 1.0\n",
      "Iteration : 200   Loss : 0.266019344329834\n",
      "Iteration : 200   Accuracy : 1.0\n",
      "Iteration : 210   Loss : 0.2642044425010681\n",
      "Iteration : 210   Accuracy : 1.0\n",
      "Iteration : 220   Loss : 0.2610826790332794\n",
      "Iteration : 220   Accuracy : 1.0\n",
      "Iteration : 230   Loss : 0.2594660818576813\n",
      "Iteration : 230   Accuracy : 1.0\n",
      "Iteration : 240   Loss : 0.25664007663726807\n",
      "Iteration : 240   Accuracy : 1.0\n",
      "Iteration : 250   Loss : 0.25585711002349854\n",
      "Iteration : 250   Accuracy : 1.0\n",
      "Iteration : 260   Loss : 0.25414416193962097\n",
      "Iteration : 260   Accuracy : 1.0\n",
      "Iteration : 270   Loss : 0.25173553824424744\n",
      "Iteration : 270   Accuracy : 1.0\n",
      "Iteration : 280   Loss : 0.24814274907112122\n",
      "Iteration : 280   Accuracy : 1.0\n",
      "Iteration : 290   Loss : 0.24648131430149078\n",
      "Iteration : 290   Accuracy : 1.0\n",
      "Iteration : 300   Loss : 0.24403896927833557\n",
      "Iteration : 300   Accuracy : 1.0\n",
      "Iteration : 310   Loss : 0.24304719269275665\n",
      "Iteration : 310   Accuracy : 1.0\n",
      "Iteration : 320   Loss : 0.24148839712142944\n",
      "Iteration : 320   Accuracy : 1.0\n",
      "Iteration : 330   Loss : 0.23800930380821228\n",
      "Iteration : 330   Accuracy : 1.0\n",
      "Iteration : 340   Loss : 0.23828797042369843\n",
      "Iteration : 340   Accuracy : 1.0\n",
      "Iteration : 350   Loss : 0.23681402206420898\n",
      "Iteration : 350   Accuracy : 1.0\n",
      "Iteration : 360   Loss : 0.2329053431749344\n",
      "Iteration : 360   Accuracy : 1.0\n",
      "Iteration : 370   Loss : 0.23109737038612366\n",
      "Iteration : 370   Accuracy : 1.0\n",
      "Iteration : 380   Loss : 0.230681374669075\n",
      "Iteration : 380   Accuracy : 1.0\n",
      "Iteration : 390   Loss : 0.22759151458740234\n",
      "Iteration : 390   Accuracy : 1.0\n",
      "Iteration : 400   Loss : 0.22619153559207916\n",
      "Iteration : 400   Accuracy : 1.0\n",
      "Iteration : 410   Loss : 0.2241678237915039\n",
      "Iteration : 410   Accuracy : 1.0\n",
      "Iteration : 420   Loss : 0.22153444588184357\n",
      "Iteration : 420   Accuracy : 1.0\n",
      "Iteration : 430   Loss : 0.22053086757659912\n",
      "Iteration : 430   Accuracy : 1.0\n",
      "Iteration : 440   Loss : 0.21831950545310974\n",
      "Iteration : 440   Accuracy : 1.0\n",
      "Iteration : 450   Loss : 0.2172761708498001\n",
      "Iteration : 450   Accuracy : 1.0\n",
      "Iteration : 460   Loss : 0.21523383259773254\n",
      "Iteration : 460   Accuracy : 1.0\n",
      "Iteration : 470   Loss : 0.21287450194358826\n",
      "Iteration : 470   Accuracy : 1.0\n",
      "Iteration : 480   Loss : 0.21161450445652008\n",
      "Iteration : 480   Accuracy : 1.0\n",
      "Iteration : 490   Loss : 0.208991140127182\n",
      "Iteration : 490   Accuracy : 1.0\n",
      "Iteration : 500   Loss : 0.20744171738624573\n",
      "Iteration : 500   Accuracy : 1.0\n",
      "Iteration : 510   Loss : 0.20659077167510986\n",
      "Iteration : 510   Accuracy : 1.0\n",
      "Iteration : 520   Loss : 0.20550785958766937\n",
      "Iteration : 520   Accuracy : 1.0\n",
      "Iteration : 530   Loss : 0.20508906245231628\n",
      "Iteration : 530   Accuracy : 1.0\n",
      "Iteration : 540   Loss : 0.2028140127658844\n",
      "Iteration : 540   Accuracy : 1.0\n",
      "Iteration : 550   Loss : 0.20220136642456055\n",
      "Iteration : 550   Accuracy : 1.0\n",
      "Iteration : 560   Loss : 0.1986982673406601\n",
      "Iteration : 560   Accuracy : 1.0\n",
      "Iteration : 570   Loss : 0.19824054837226868\n",
      "Iteration : 570   Accuracy : 1.0\n",
      "Iteration : 580   Loss : 0.1967732012271881\n",
      "Iteration : 580   Accuracy : 1.0\n",
      "Iteration : 590   Loss : 0.1948247253894806\n",
      "Iteration : 590   Accuracy : 1.0\n",
      "Iteration : 600   Loss : 0.19313667714595795\n",
      "Iteration : 600   Accuracy : 1.0\n",
      "Iteration : 610   Loss : 0.1912570595741272\n",
      "Iteration : 610   Accuracy : 1.0\n",
      "Iteration : 620   Loss : 0.19011148810386658\n",
      "Iteration : 620   Accuracy : 1.0\n",
      "Iteration : 630   Loss : 0.1894284337759018\n",
      "Iteration : 630   Accuracy : 1.0\n",
      "Iteration : 640   Loss : 0.188803032040596\n",
      "Iteration : 640   Accuracy : 1.0\n",
      "Iteration : 650   Loss : 0.18723440170288086\n",
      "Iteration : 650   Accuracy : 1.0\n",
      "Iteration : 660   Loss : 0.18521438539028168\n",
      "Iteration : 660   Accuracy : 1.0\n",
      "Iteration : 670   Loss : 0.1836736649274826\n",
      "Iteration : 670   Accuracy : 1.0\n",
      "Iteration : 680   Loss : 0.1824294477701187\n",
      "Iteration : 680   Accuracy : 1.0\n",
      "Iteration : 690   Loss : 0.18345387279987335\n",
      "Iteration : 690   Accuracy : 1.0\n",
      "Iteration : 700   Loss : 0.18049576878547668\n",
      "Iteration : 700   Accuracy : 1.0\n",
      "Iteration : 710   Loss : 0.17864760756492615\n",
      "Iteration : 710   Accuracy : 1.0\n",
      "Iteration : 720   Loss : 0.17666278779506683\n",
      "Iteration : 720   Accuracy : 1.0\n",
      "Iteration : 730   Loss : 0.17676642537117004\n",
      "Iteration : 730   Accuracy : 1.0\n",
      "Iteration : 740   Loss : 0.17592670023441315\n",
      "Iteration : 740   Accuracy : 1.0\n",
      "Iteration : 750   Loss : 0.17239253222942352\n",
      "Iteration : 750   Accuracy : 1.0\n",
      "Iteration : 760   Loss : 0.17082397639751434\n",
      "Iteration : 760   Accuracy : 1.0\n",
      "Iteration : 770   Loss : 0.1700625717639923\n",
      "Iteration : 770   Accuracy : 1.0\n",
      "Iteration : 780   Loss : 0.16939379274845123\n",
      "Iteration : 780   Accuracy : 1.0\n",
      "Iteration : 790   Loss : 0.16749544441699982\n",
      "Iteration : 790   Accuracy : 1.0\n",
      "Iteration : 800   Loss : 0.16646546125411987\n",
      "Iteration : 800   Accuracy : 1.0\n",
      "Iteration : 810   Loss : 0.16522373259067535\n",
      "Iteration : 810   Accuracy : 1.0\n",
      "Iteration : 820   Loss : 0.16508397459983826\n",
      "Iteration : 820   Accuracy : 1.0\n",
      "Iteration : 830   Loss : 0.16486287117004395\n",
      "Iteration : 830   Accuracy : 1.0\n",
      "Iteration : 840   Loss : 0.16406066715717316\n",
      "Iteration : 840   Accuracy : 1.0\n",
      "Iteration : 850   Loss : 0.1625937521457672\n",
      "Iteration : 850   Accuracy : 1.0\n",
      "Iteration : 860   Loss : 0.16045290231704712\n",
      "Iteration : 860   Accuracy : 1.0\n",
      "Iteration : 870   Loss : 0.1588171124458313\n",
      "Iteration : 870   Accuracy : 1.0\n",
      "Iteration : 880   Loss : 0.15750311315059662\n",
      "Iteration : 880   Accuracy : 1.0\n",
      "Iteration : 890   Loss : 0.15621232986450195\n",
      "Iteration : 890   Accuracy : 1.0\n",
      "Iteration : 900   Loss : 0.15624286234378815\n",
      "Iteration : 900   Accuracy : 1.0\n",
      "Iteration : 910   Loss : 0.15510891377925873\n",
      "Iteration : 910   Accuracy : 1.0\n",
      "Iteration : 920   Loss : 0.15364669263362885\n",
      "Iteration : 920   Accuracy : 1.0\n",
      "Iteration : 930   Loss : 0.15210404992103577\n",
      "Iteration : 930   Accuracy : 1.0\n",
      "Iteration : 940   Loss : 0.15127883851528168\n",
      "Iteration : 940   Accuracy : 1.0\n",
      "Iteration : 950   Loss : 0.15030038356781006\n",
      "Iteration : 950   Accuracy : 1.0\n",
      "Iteration : 960   Loss : 0.14897014200687408\n",
      "Iteration : 960   Accuracy : 1.0\n",
      "Iteration : 970   Loss : 0.1485183835029602\n",
      "Iteration : 970   Accuracy : 1.0\n",
      "Iteration : 980   Loss : 0.14710040390491486\n",
      "Iteration : 980   Accuracy : 1.0\n",
      "Iteration : 990   Loss : 0.14692887663841248\n",
      "Iteration : 990   Accuracy : 1.0\n",
      "Iteration : 1000   Loss : 0.14547620713710785\n",
      "Iteration : 1000   Accuracy : 1.0\n",
      "Iteration : 1010   Loss : 0.14536535739898682\n",
      "Iteration : 1010   Accuracy : 1.0\n",
      "Iteration : 1020   Loss : 0.14299602806568146\n",
      "Iteration : 1020   Accuracy : 1.0\n",
      "Iteration : 1030   Loss : 0.14314498007297516\n",
      "Iteration : 1030   Accuracy : 1.0\n",
      "Iteration : 1040   Loss : 0.1425171047449112\n",
      "Iteration : 1040   Accuracy : 1.0\n",
      "Iteration : 1050   Loss : 0.14035622775554657\n",
      "Iteration : 1050   Accuracy : 1.0\n",
      "Iteration : 1060   Loss : 0.14012804627418518\n",
      "Iteration : 1060   Accuracy : 1.0\n",
      "Iteration : 1070   Loss : 0.1380060911178589\n",
      "Iteration : 1070   Accuracy : 1.0\n",
      "Iteration : 1080   Loss : 0.13764384388923645\n",
      "Iteration : 1080   Accuracy : 1.0\n",
      "Iteration : 1090   Loss : 0.13652536273002625\n",
      "Iteration : 1090   Accuracy : 1.0\n",
      "Iteration : 1100   Loss : 0.13631364703178406\n",
      "Iteration : 1100   Accuracy : 1.0\n",
      "Iteration : 1110   Loss : 0.1349884420633316\n",
      "Iteration : 1110   Accuracy : 1.0\n",
      "Iteration : 1120   Loss : 0.13411477208137512\n",
      "Iteration : 1120   Accuracy : 1.0\n",
      "Iteration : 1130   Loss : 0.1332513689994812\n",
      "Iteration : 1130   Accuracy : 1.0\n",
      "Iteration : 1140   Loss : 0.132253035902977\n",
      "Iteration : 1140   Accuracy : 1.0\n",
      "Iteration : 1150   Loss : 0.1305750161409378\n",
      "Iteration : 1150   Accuracy : 1.0\n",
      "Iteration : 1160   Loss : 0.12998327612876892\n",
      "Iteration : 1160   Accuracy : 1.0\n",
      "Iteration : 1170   Loss : 0.12970387935638428\n",
      "Iteration : 1170   Accuracy : 1.0\n",
      "Iteration : 1180   Loss : 0.12877613306045532\n",
      "Iteration : 1180   Accuracy : 1.0\n",
      "Iteration : 1190   Loss : 0.1277683675289154\n",
      "Iteration : 1190   Accuracy : 1.0\n",
      "Iteration : 1200   Loss : 0.12790460884571075\n",
      "Iteration : 1200   Accuracy : 1.0\n",
      "Iteration : 1210   Loss : 0.12668217718601227\n",
      "Iteration : 1210   Accuracy : 1.0\n",
      "Iteration : 1220   Loss : 0.12485277652740479\n",
      "Iteration : 1220   Accuracy : 1.0\n",
      "Iteration : 1230   Loss : 0.12536825239658356\n",
      "Iteration : 1230   Accuracy : 1.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b581d61c9f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(10000+1):\n",
    "    pointnet.zero_grad()\n",
    "\n",
    "    input_data, labels = data_sampler(batch_size, num_points)\n",
    "\n",
    "    output = pointnet(input_data)\n",
    "    output = nn.Sigmoid()(output)\n",
    "\n",
    "    error = criterion(output, labels)\n",
    "    error.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output[output > 0.5] = 1\n",
    "        output[output < 0.5] = 0\n",
    "        accuracy = (output==labels).sum().item()/batch_size\n",
    "\n",
    "    loss_list.append(error.item())\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        print('Iteration : {}   Loss : {}'.format(iteration, error.item()))\n",
    "        print('Iteration : {}   Accuracy : {}'.format(iteration, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}